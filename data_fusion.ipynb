{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd68ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfbad333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vehicles.VehicleNumber</th>\n",
       "      <th>Vehicles.DamageLevel</th>\n",
       "      <th>Vehicles.ExplosionType</th>\n",
       "      <th>Vehicles.FireType</th>\n",
       "      <th>Vehicles.SerialNumber</th>\n",
       "      <th>Vehicles.Make</th>\n",
       "      <th>Vehicles.Model</th>\n",
       "      <th>Vehicles.NumberOfEngines</th>\n",
       "      <th>Vehicles.RegistrationNumber</th>\n",
       "      <th>Vehicles.FlightOperationType</th>\n",
       "      <th>...</th>\n",
       "      <th>Country</th>\n",
       "      <th>EventDate</th>\n",
       "      <th>State</th>\n",
       "      <th>Agency</th>\n",
       "      <th>EventType</th>\n",
       "      <th>AirportId</th>\n",
       "      <th>AirportName</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>TotalInjuryCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22157</th>\n",
       "      <td>1</td>\n",
       "      <td>destroyed</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>32r-8513011</td>\n",
       "      <td>piper</td>\n",
       "      <td>pa-32r-301</td>\n",
       "      <td>1</td>\n",
       "      <td>n69190</td>\n",
       "      <td>pers</td>\n",
       "      <td>...</td>\n",
       "      <td>usa</td>\n",
       "      <td>2003-10-22 00:00:00</td>\n",
       "      <td>tn</td>\n",
       "      <td>ntsb</td>\n",
       "      <td>acc</td>\n",
       "      <td>kbna</td>\n",
       "      <td>nashville international</td>\n",
       "      <td>35.701946</td>\n",
       "      <td>-85.397224</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12080</th>\n",
       "      <td>2</td>\n",
       "      <td>substantial</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>961007</td>\n",
       "      <td>yakovlev</td>\n",
       "      <td>yak-55m</td>\n",
       "      <td>1</td>\n",
       "      <td>n521bc</td>\n",
       "      <td>pers</td>\n",
       "      <td>...</td>\n",
       "      <td>usa</td>\n",
       "      <td>2011-08-20 13:15:00</td>\n",
       "      <td>nj</td>\n",
       "      <td>ntsb</td>\n",
       "      <td>acc</td>\n",
       "      <td>n81</td>\n",
       "      <td>hammonton</td>\n",
       "      <td>39.659168</td>\n",
       "      <td>-74.752220</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18553</th>\n",
       "      <td>1</td>\n",
       "      <td>substantial</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>48-21</td>\n",
       "      <td>nanchang china</td>\n",
       "      <td>cj-6</td>\n",
       "      <td>1</td>\n",
       "      <td>n8120h</td>\n",
       "      <td>pers</td>\n",
       "      <td>...</td>\n",
       "      <td>usa</td>\n",
       "      <td>2006-07-02 18:50:00</td>\n",
       "      <td>wa</td>\n",
       "      <td>ntsb</td>\n",
       "      <td>acc</td>\n",
       "      <td>s97</td>\n",
       "      <td>anderson field</td>\n",
       "      <td>48.104721</td>\n",
       "      <td>-119.730003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3913</th>\n",
       "      <td>1</td>\n",
       "      <td>substantial</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>17251247</td>\n",
       "      <td>cessna</td>\n",
       "      <td>172</td>\n",
       "      <td>1</td>\n",
       "      <td>n5347t</td>\n",
       "      <td>pers</td>\n",
       "      <td>...</td>\n",
       "      <td>usa</td>\n",
       "      <td>2019-09-10 13:30:00</td>\n",
       "      <td>nd</td>\n",
       "      <td>ntsb</td>\n",
       "      <td>acc</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>47.378612</td>\n",
       "      <td>-103.589721</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>1</td>\n",
       "      <td>substantial</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>lm-79</td>\n",
       "      <td>beech</td>\n",
       "      <td>65-a90-1</td>\n",
       "      <td>2</td>\n",
       "      <td>n80y</td>\n",
       "      <td>inst</td>\n",
       "      <td>...</td>\n",
       "      <td>usa</td>\n",
       "      <td>2023-05-18 12:59:00</td>\n",
       "      <td>wv</td>\n",
       "      <td>ntsb</td>\n",
       "      <td>acc</td>\n",
       "      <td>w99</td>\n",
       "      <td>grant county airport</td>\n",
       "      <td>38.995833</td>\n",
       "      <td>-79.148611</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19908</th>\n",
       "      <td>1</td>\n",
       "      <td>substantial</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>0530</td>\n",
       "      <td>cirrus design corp.</td>\n",
       "      <td>sr-22</td>\n",
       "      <td>1</td>\n",
       "      <td>n3452l</td>\n",
       "      <td>pers</td>\n",
       "      <td>...</td>\n",
       "      <td>usa</td>\n",
       "      <td>2005-06-30 16:40:00</td>\n",
       "      <td>ny</td>\n",
       "      <td>ntsb</td>\n",
       "      <td>acc</td>\n",
       "      <td>hpn</td>\n",
       "      <td>westchester county</td>\n",
       "      <td>41.200000</td>\n",
       "      <td>-73.961669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8565</th>\n",
       "      <td>1</td>\n",
       "      <td>substantial</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>19000133</td>\n",
       "      <td>embraer</td>\n",
       "      <td>erj 190 100 igw</td>\n",
       "      <td>2</td>\n",
       "      <td>n953uw</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>usa</td>\n",
       "      <td>2015-02-09 23:07:00</td>\n",
       "      <td>tx</td>\n",
       "      <td>ntsb</td>\n",
       "      <td>acc</td>\n",
       "      <td>iah</td>\n",
       "      <td>george bush intercontinental</td>\n",
       "      <td>29.984443</td>\n",
       "      <td>-95.341392</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5945</th>\n",
       "      <td>1</td>\n",
       "      <td>substantial</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>aa5b0937</td>\n",
       "      <td>gulfstream american corp</td>\n",
       "      <td>aa 5b</td>\n",
       "      <td>1</td>\n",
       "      <td>n28005</td>\n",
       "      <td>pers</td>\n",
       "      <td>...</td>\n",
       "      <td>usa</td>\n",
       "      <td>2017-08-28 10:06:00</td>\n",
       "      <td>il</td>\n",
       "      <td>ntsb</td>\n",
       "      <td>acc</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>37.673053</td>\n",
       "      <td>-89.259162</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13838</th>\n",
       "      <td>1</td>\n",
       "      <td>substantial</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>1199</td>\n",
       "      <td>univair aircraft corporation</td>\n",
       "      <td>415-c</td>\n",
       "      <td>1</td>\n",
       "      <td>n93876</td>\n",
       "      <td>pers</td>\n",
       "      <td>...</td>\n",
       "      <td>usa</td>\n",
       "      <td>2010-04-12 11:30:00</td>\n",
       "      <td>mi</td>\n",
       "      <td>ntsb</td>\n",
       "      <td>acc</td>\n",
       "      <td>pvt</td>\n",
       "      <td>None</td>\n",
       "      <td>43.125000</td>\n",
       "      <td>-83.450279</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10215</th>\n",
       "      <td>1</td>\n",
       "      <td>destroyed</td>\n",
       "      <td>none</td>\n",
       "      <td>on-ground</td>\n",
       "      <td>32r-7680452</td>\n",
       "      <td>piper</td>\n",
       "      <td>pa-32r-300</td>\n",
       "      <td>1</td>\n",
       "      <td>n4489f</td>\n",
       "      <td>pers</td>\n",
       "      <td>...</td>\n",
       "      <td>usa</td>\n",
       "      <td>2013-05-27 18:05:00</td>\n",
       "      <td>ga</td>\n",
       "      <td>ntsb</td>\n",
       "      <td>acc</td>\n",
       "      <td>mcn</td>\n",
       "      <td>middle georgia regional</td>\n",
       "      <td>32.663887</td>\n",
       "      <td>-83.603057</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Vehicles.VehicleNumber Vehicles.DamageLevel Vehicles.ExplosionType  \\\n",
       "22157                       1            destroyed                   none   \n",
       "12080                       2          substantial                   none   \n",
       "18553                       1          substantial                   none   \n",
       "3913                        1          substantial                   none   \n",
       "619                         1          substantial                   none   \n",
       "19908                       1          substantial                   none   \n",
       "8565                        1          substantial                   none   \n",
       "5945                        1          substantial                   none   \n",
       "13838                       1          substantial                   none   \n",
       "10215                       1            destroyed                   none   \n",
       "\n",
       "      Vehicles.FireType Vehicles.SerialNumber                 Vehicles.Make  \\\n",
       "22157              none           32r-8513011                         piper   \n",
       "12080              none                961007                      yakovlev   \n",
       "18553              none                 48-21                nanchang china   \n",
       "3913               none              17251247                        cessna   \n",
       "619                none                 lm-79                         beech   \n",
       "19908              none                  0530           cirrus design corp.   \n",
       "8565               none              19000133                       embraer   \n",
       "5945               none              aa5b0937      gulfstream american corp   \n",
       "13838              none                  1199  univair aircraft corporation   \n",
       "10215         on-ground           32r-7680452                         piper   \n",
       "\n",
       "        Vehicles.Model  Vehicles.NumberOfEngines Vehicles.RegistrationNumber  \\\n",
       "22157       pa-32r-301                         1                      n69190   \n",
       "12080          yak-55m                         1                      n521bc   \n",
       "18553             cj-6                         1                      n8120h   \n",
       "3913               172                         1                      n5347t   \n",
       "619           65-a90-1                         2                        n80y   \n",
       "19908            sr-22                         1                      n3452l   \n",
       "8565   erj 190 100 igw                         2                      n953uw   \n",
       "5945             aa 5b                         1                      n28005   \n",
       "13838            415-c                         1                      n93876   \n",
       "10215       pa-32r-300                         1                      n4489f   \n",
       "\n",
       "      Vehicles.FlightOperationType  ... Country           EventDate  State  \\\n",
       "22157                         pers  ...     usa 2003-10-22 00:00:00     tn   \n",
       "12080                         pers  ...     usa 2011-08-20 13:15:00     nj   \n",
       "18553                         pers  ...     usa 2006-07-02 18:50:00     wa   \n",
       "3913                          pers  ...     usa 2019-09-10 13:30:00     nd   \n",
       "619                           inst  ...     usa 2023-05-18 12:59:00     wv   \n",
       "19908                         pers  ...     usa 2005-06-30 16:40:00     ny   \n",
       "8565                          None  ...     usa 2015-02-09 23:07:00     tx   \n",
       "5945                          pers  ...     usa 2017-08-28 10:06:00     il   \n",
       "13838                         pers  ...     usa 2010-04-12 11:30:00     mi   \n",
       "10215                         pers  ...     usa 2013-05-27 18:05:00     ga   \n",
       "\n",
       "      Agency EventType AirportId                   AirportName   Latitude  \\\n",
       "22157   ntsb       acc      kbna       nashville international  35.701946   \n",
       "12080   ntsb       acc       n81                     hammonton  39.659168   \n",
       "18553   ntsb       acc       s97                anderson field  48.104721   \n",
       "3913    ntsb       acc      None                          None  47.378612   \n",
       "619     ntsb       acc       w99          grant county airport  38.995833   \n",
       "19908   ntsb       acc       hpn            westchester county  41.200000   \n",
       "8565    ntsb       acc       iah  george bush intercontinental  29.984443   \n",
       "5945    ntsb       acc      None                          None  37.673053   \n",
       "13838   ntsb       acc       pvt                          None  43.125000   \n",
       "10215   ntsb       acc       mcn       middle georgia regional  32.663887   \n",
       "\n",
       "        Longitude TotalInjuryCount  \n",
       "22157  -85.397224                1  \n",
       "12080  -74.752220                2  \n",
       "18553 -119.730003                0  \n",
       "3913  -103.589721                1  \n",
       "619    -79.148611                0  \n",
       "19908  -73.961669                1  \n",
       "8565   -95.341392                1  \n",
       "5945   -89.259162                2  \n",
       "13838  -83.450279                0  \n",
       "10215  -83.603057                2  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"data_sources/filtered/ntsb.pkl\")\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a4057d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_indices count (60%): 14041\n",
      "Number of NaNs assigned: 7020\n",
      "Total NaNs in 'State' column: 7050\n"
     ]
    }
   ],
   "source": [
    "ntsb_og = df.copy()\n",
    "ntsb_copied = df.copy()\n",
    "ntsb_copied = ntsb_copied.rename(columns=\n",
    "    {\"EventDate\": \"Date\", \n",
    "    \"NtsbNumber\": \"ID\", \n",
    "    \"State\": \"Location\"}) # for schema matching \n",
    "\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "\n",
    "n = len(ntsb_copied)\n",
    "\n",
    "# select 60% indices for later use\n",
    "n_forty = int(np.floor(0.6 * n))\n",
    "random_indices = np.random.choice(ntsb_copied.index, n_forty, replace=False)\n",
    "print(f\"random_indices count (60%): {len(random_indices)}\")  # Should be ~0.6 * n\n",
    "\n",
    "# select half of these indices to assign NaN in 'ID'\n",
    "n_missing = int(np.floor(0.5 * n_forty))\n",
    "missing_indices = np.random.choice(random_indices, n_missing, replace=False)\n",
    "\n",
    "# assign NaN only to these missing_indices, in original to then match\n",
    "ntsb_og.loc[missing_indices, \"State\"] = np.nan  # for slot filling\n",
    "\n",
    "print(f\"Number of NaNs assigned: {len(missing_indices)}\")\n",
    "\n",
    "# count total NaNs in 'ID' (including existing NaNs)\n",
    "total_nans = ntsb_og['State'].isna().sum()\n",
    "print(f\"Total NaNs in 'State' column: {total_nans}\")\n",
    "\n",
    "conflict_indices = np.setdiff1d(random_indices, missing_indices)\n",
    "\n",
    "# transform strings for conflict resolution\n",
    "for index, row in ntsb_copied.iterrows():\n",
    "    if index in conflict_indices:\n",
    "        airport = row[\"AirportName\"]\n",
    "        if pd.notna(airport):\n",
    "            result = ' '.join([word[0] + '.' for word in airport.split()])\n",
    "            ntsb_copied.loc[index, \"AirportName\"] = result\n",
    "\n",
    "ntsb_og.to_pickle(\"data_sources/to_fuse/ntsb_og.pkl\")\n",
    "ntsb_copied.to_pickle(\"data_sources/to_fuse/ntsb_copied.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf9ae41",
   "metadata": {},
   "source": [
    "### Fuse NTSB with its dupe for strategy implemenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93bbd76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in 'State' (before fusion): 7050\n",
      "Abbreviated AirportName (before fusion): 5125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging entries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23403/23403 [00:56<00:00, 413.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final fused dataset saved:\n",
      " â€¢ accident_weather_fused_final.pkl\n",
      " â€¢ accident_weather_fused_final.csv\n",
      "Final row count: 23400 (original: 23403, added: 120)\n",
      "NaN in 'State' (after fusion): 47\n",
      "'State' values filled during fusion: 7003 (99.33%)\n",
      "Deduplication removed: 3 rows (0.01%)\n",
      "Abbreviated AirportName (after fusion): 20\n",
      "Resolved AirportName values: 5105 (99.61%)\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "# Load datasets\n",
    "ntsb_og = pd.read_pickle('data_sources/to_fuse/ntsb_og.pkl')\n",
    "ntsb_copied = pd.read_pickle('data_sources/to_fuse/ntsb_copied.pkl')  # adjust if needed\n",
    "\n",
    "# NaN count BEFORE fusion\n",
    "id_nans_before = ntsb_og['State'].isna().sum()\n",
    "print(f\"NaN in 'State' (before fusion): {id_nans_before}\")\n",
    "\n",
    "# Count abbreviated AirportName BEFORE fusion (optional)\n",
    "abbrev_pattern = r'^([a-zA-Z]\\.\\s*)+$'\n",
    "airport_abbrev_before = ntsb_copied['AirportName'].fillna('').str.match(abbrev_pattern).sum()\n",
    "print(f\"Abbreviated AirportName (before fusion): {airport_abbrev_before}\")\n",
    "\n",
    "# Normalize column names in ntsb_copied\n",
    "ntsb_copied = ntsb_copied.rename(columns={\n",
    "    \"Date\": \"EventDate\", \n",
    "    \"ID\": \"NtsbNumber\", \n",
    "    \"Location\": \"State\"\n",
    "})\n",
    "\n",
    "# Drop NtsbNumber\n",
    "ntsb_og = ntsb_og.drop(columns=['NtsbNumber'], errors='ignore')\n",
    "ntsb_copied = ntsb_copied.drop(columns=['NtsbNumber'], errors='ignore')\n",
    "\n",
    "# Helper: fuzzy similarity for airport names\n",
    "def fuzzy_match_airport(row, df_ref):\n",
    "    candidates = df_ref[\n",
    "        (df_ref['EventDate'] == row['EventDate']) &\n",
    "        (df_ref['Vehicles.RegistrationNumber'] == row['Vehicles.RegistrationNumber']) &\n",
    "        (df_ref['Vehicles.SerialNumber'] == row['Vehicles.SerialNumber'])\n",
    "    ]\n",
    "    if candidates.empty or pd.isna(row['AirportName']):\n",
    "        return None\n",
    "    best = candidates['AirportName'].dropna().apply(lambda x: fuzz.partial_ratio(x, row['AirportName']))\n",
    "    if not best.empty and best.max() >= 80:\n",
    "        return candidates.iloc[best.idxmax()]\n",
    "    return None\n",
    "\n",
    "# Merge logic\n",
    "fused_rows = []\n",
    "unmatched_rows = []\n",
    "\n",
    "for _, row in tqdm(ntsb_copied.iterrows(), total=len(ntsb_copied), desc=\"Merging entries\"):\n",
    "    match = ntsb_og[\n",
    "        (ntsb_og['EventDate'] == row['EventDate']) &\n",
    "        (ntsb_og['Vehicles.RegistrationNumber'] == row['Vehicles.RegistrationNumber']) &\n",
    "        (ntsb_og['Vehicles.SerialNumber'] == row['Vehicles.SerialNumber'])\n",
    "    ]\n",
    "\n",
    "    if match.empty:\n",
    "        unmatched_rows.append(row)\n",
    "        continue\n",
    "\n",
    "    merged = match.iloc[0].copy()\n",
    "\n",
    "    # Slot Filling: if merged[col] is NA and row[col] is not, use row[col]\n",
    "    for col in ntsb_copied.columns:\n",
    "        if col in merged.index and pd.isna(merged[col]) and pd.notna(row[col]):\n",
    "            merged[col] = row[col]\n",
    "    \n",
    "    # Conflict resolution: prioritize df unless row[col] is clearly more complete\n",
    "    if pd.notna(row['AirportName']) and pd.notna(merged['AirportName']):\n",
    "        if len(row['AirportName']) > len(merged['AirportName']):  # assume longer name is better\n",
    "            merged['AirportName'] = row['AirportName']\n",
    "\n",
    "    fused_rows.append(merged)\n",
    "\n",
    "# Construct final fused dataframe\n",
    "fused_df = pd.DataFrame(fused_rows)\n",
    "\n",
    "# Reattach unmatched rows (optional)\n",
    "fused_df = pd.concat([fused_df, pd.DataFrame(unmatched_rows)], ignore_index=True)\n",
    "\n",
    "before_dedup = len(fused_df)\n",
    "# Deduplicate\n",
    "fused_df = fused_df.drop_duplicates(subset=[\n",
    "    'EventDate', 'Vehicles.SerialNumber', 'Vehicles.RegistrationNumber'\n",
    "])\n",
    "\n",
    "# Save final fused result\n",
    "fused_df.to_pickle('data_sources/fused/ntsb_fused.pkl')\n",
    "fused_df.to_csv('data_sources/fused/ntsb_fused.csv', index=False)\n",
    "\n",
    "print(\"Final fused dataset saved:\")\n",
    "print(\" â€¢ accident_weather_fused_final.pkl\")\n",
    "print(\" â€¢ accident_weather_fused_final.csv\")\n",
    "print(f\"Final row count: {len(fused_df)} (original: {len(df)}, added: {len(unmatched_rows)})\")\n",
    "\n",
    "# NaN count AFTER fusion\n",
    "id_nans_after = fused_df['State'].isna().sum()\n",
    "print(f\"NaN in 'State' (after fusion): {id_nans_after}\")\n",
    "filled_count = id_nans_before - id_nans_after\n",
    "nan_percent = filled_count / id_nans_before * 100\n",
    "print(f\"'State' values filled during fusion: {filled_count} ({nan_percent:.2f}%)\")\n",
    "\n",
    "# Deduplication stats\n",
    "after_dedup = len(fused_df)\n",
    "dedup_removed = before_dedup - after_dedup\n",
    "dedup_percent = (dedup_removed / before_dedup) * 100\n",
    "print(f\"Deduplication removed: {dedup_removed} rows ({dedup_percent:.2f}%)\")\n",
    "\n",
    "# Count abbreviated AirportName AFTER fusion\n",
    "abbrev_pattern = r'^([a-zA-Z]\\.\\s*)+$'\n",
    "airport_abbrev_after = fused_df['AirportName'].fillna('').str.match(abbrev_pattern).sum()\n",
    "print(f\"Abbreviated AirportName (after fusion): {airport_abbrev_after}\")\n",
    "resolved_count = airport_abbrev_before - airport_abbrev_after\n",
    "resolved_percent = resolved_count / airport_abbrev_before * 100\n",
    "print(f\"Resolved AirportName values: {resolved_count} ({resolved_percent:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f60e6e",
   "metadata": {},
   "source": [
    "## Weather Data Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "295f0cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched 20858 of 23403 accidents (89.1%)\n",
      "\n",
      "Time difference (min) for matched rows:\n",
      "count    20858.00\n",
      "mean        14.80\n",
      "std         10.81\n",
      "min          0.00\n",
      "25%          5.00\n",
      "50%         15.00\n",
      "75%         25.00\n",
      "max         58.00\n",
      "Name: wx_time_diff, dtype: float64\n",
      "\n",
      "Spatial deltas (deg lat/lon) for matched rows:\n",
      "              lat         lon\n",
      "count  20858.0000  20858.0000\n",
      "mean       0.0000      0.0000\n",
      "std        0.0010      0.0010\n",
      "min        0.0000      0.0000\n",
      "25%        0.0000      0.0000\n",
      "50%        0.0000      0.0000\n",
      "75%        0.0000      0.0000\n",
      "max        0.0753      0.0881\n"
     ]
    }
   ],
   "source": [
    "# spatial & temporal thresholds\n",
    "LAT_LON_EPS   = 0.10       # â‰ˆ 11 km at mid-latitudes\n",
    "MAX_TIME_DIFF = pd.Timedelta('3h')   # reject candidates > 3 h away\n",
    "\n",
    "# --- 1. Load data -------------------------------------------------------------\n",
    "ntsb_path    = Path(\"data_sources/filtered/ntsb.pkl\")\n",
    "weather_path = Path(\"data_sources/filtered/weather.pkl\")\n",
    "\n",
    "ntsb    = pd.read_pickle(ntsb_path)\n",
    "weather = pd.read_pickle(weather_path)\n",
    "\n",
    "# ensure correct dtypes\n",
    "ntsb[\"EventDate\"] = pd.to_datetime(ntsb[\"EventDate\"], errors=\"coerce\")\n",
    "weather[\"time\"]   = pd.to_datetime(weather[\"time\"],   errors=\"coerce\")\n",
    "\n",
    "# --- 2. Blocking on event *date* ---------------------------------------------\n",
    "ntsb[\"event_day\"]    = ntsb[\"EventDate\"].dt.date\n",
    "weather[\"weather_day\"] = weather[\"time\"].dt.date\n",
    "\n",
    "weather_by_day = {d: w.reset_index(drop=True)\n",
    "                  for d, w in weather.groupby(\"weather_day\")}\n",
    "\n",
    "# --- 3. Similarity matching & temporal precedence -----------------------------\n",
    "best_rows = []       # stores best-matching weather rows (or None)\n",
    "\n",
    "for _, acc in ntsb.iterrows():\n",
    "    day_candidates = weather_by_day.get(acc[\"event_day\"], pd.DataFrame())\n",
    "    if day_candidates.empty:\n",
    "        best_rows.append(None); continue\n",
    "    \n",
    "    # coarse spatial filter  |lat/lon diff| < LAT_LON_EPS\n",
    "    spatial = day_candidates[\n",
    "        (day_candidates[\"time\"].notna()) &\n",
    "        (day_candidates[\"AccidentID\"].notna()) &       # keeps malformed rows out\n",
    "        (day_candidates[\"AccidentID\"].str.contains('_'))  # quick sanity\n",
    "    ].copy()\n",
    "\n",
    "    spatial = spatial[\n",
    "        (np.abs(spatial[\"AccidentID\"].str.split('_').str[-2].astype(float) - acc[\"Latitude\" ] ) < LAT_LON_EPS) &\n",
    "        (np.abs(spatial[\"AccidentID\"].str.split('_').str[-1].astype(float) - acc[\"Longitude\"]) < LAT_LON_EPS)\n",
    "    ]\n",
    "\n",
    "    if spatial.empty:\n",
    "        best_rows.append(None); continue\n",
    "    \n",
    "    # temporal distance to the accident moment\n",
    "    spatial[\"time_diff\"] = (spatial[\"time\"] - acc[\"EventDate\"]).abs()\n",
    "    \n",
    "    # keep the closest hour that is still within MAX_TIME_DIFF\n",
    "    spatial = spatial[spatial[\"time_diff\"] <= MAX_TIME_DIFF]\n",
    "    \n",
    "    best_rows.append(spatial.nsmallest(1, \"time_diff\").iloc[0] if not spatial.empty else None)\n",
    "\n",
    "# --- 4. Assemble the fused dataset -------------------------------------------\n",
    "weather_match_df = pd.DataFrame.from_records(\n",
    "    [row.to_dict() if row is not None else {}          # convert None into an empty dict {}\n",
    "     for row in best_rows],\n",
    "    index=ntsb.index                                   # keeps row-alignment\n",
    ")\n",
    "\n",
    "accident_weather = pd.concat(\n",
    "    [ntsb.reset_index(drop=True),\n",
    "     weather_match_df.add_prefix(\"wx_\")],              # prefix to avoid clashes\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# --- 5. Quick diagnostics -----------------------------------------------------\n",
    "total_accidents = len(ntsb)\n",
    "matched         = accident_weather[\"wx_time\"].notna().sum()\n",
    "print(f\"Matched {matched} of {total_accidents} accidents \"\n",
    "      f\"({matched / total_accidents:.1%})\")\n",
    "\n",
    "if matched:\n",
    "    print(\"\\nTime difference (min) for matched rows:\")\n",
    "    print((accident_weather.loc[accident_weather.wx_time.notna(), \"wx_time_diff\"]\n",
    "           .dt.total_seconds().div(60)\n",
    "           .describe().round(2)))\n",
    "\n",
    "    print(\"\\nSpatial deltas (deg lat/lon) for matched rows:\")\n",
    "    lat_delta = np.abs(accident_weather[\"Latitude\"] - accident_weather[\"wx_AccidentID\"]\n",
    "                       .str.split('_').str[-2].astype(float))\n",
    "    lon_delta = np.abs(accident_weather[\"Longitude\"] - accident_weather[\"wx_AccidentID\"]\n",
    "                       .str.split('_').str[-1].astype(float))\n",
    "    print(pd.concat({\"lat\": lat_delta, \"lon\": lon_delta}, axis=1).describe().round(4))\n",
    "\n",
    "accident_weather.drop(columns=[\"event_day\",\"wx_AccidentID\",\"wx_weather_day\"], errors='ignore', inplace=True)\n",
    "accident_weather.to_pickle(\"data_sources/fused/accident_weather.pkl\")\n",
    "accident_weather.to_csv(\"data_sources/fused/accident_weather.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c4e03a",
   "metadata": {},
   "source": [
    "## Matched Aircraft Data Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "592f1fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion complete. Enriched dataset saved to: data_sources/fused/accident_weather_enriched.pkl\n",
      "\n",
      "--- Matching Statistics ---\n",
      "Total records in original dataset: 23403\n",
      "Total records matched with binding CSV: 4962\n",
      "Total unmatched records: 18441\n",
      "Match percentage: 21.20%\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "accident_weather_path = 'data_sources/fused/accident_weather.pkl'\n",
    "matched_results_path = 'data_sources/binding/matched_results.csv'\n",
    "\n",
    "accident_weather_df = pd.read_pickle(accident_weather_path)\n",
    "matched_results_df = pd.read_csv(matched_results_path)\n",
    "\n",
    "\n",
    "def clean_text(s):\n",
    "    \"\"\" Normalizzazione del testo: rimozione di caratteri speciali, lowercase e spazi extra. \"\"\"\n",
    "    return re.sub(r'\\W+', ' ', str(s)).lower().strip()\n",
    "\n",
    "# Pulizia dei dati\n",
    "accident_weather_df['Vehicles.Model'] = accident_weather_df['Vehicles.Model'].apply(clean_text)\n",
    "accident_weather_df['Vehicles.Make'] = accident_weather_df['Vehicles.Make'].apply(clean_text)\n",
    "\n",
    "# Normalize casing for matching\n",
    "matched_results_df['NtsbNumber'] = matched_results_df['NtsbNumber'].str.lower()\n",
    "matched_results_df['EventDate'] = pd.to_datetime(matched_results_df['EventDate'], errors='coerce')\n",
    "matched_results_df['Vehicles.SerialNumber'] = matched_results_df['Vehicles.SerialNumber'].str.lower()\n",
    "matched_results_df['Vehicles.RegistrationNumber'] = matched_results_df['Vehicles.RegistrationNumber'].str.lower()\n",
    "matched_results_df['Vehicles.Make'] = matched_results_df['Vehicles.Make'].str.lower()\n",
    "matched_results_df['Vehicles.Model'] = matched_results_df['Vehicles.Model'].str.lower()\n",
    "\n",
    "matched_results_df.drop(columns=[\"JW_Score\",\"LEV_Score\",\"Jac_Score\",\"SimilarityScore\",\"Matched_Aircraft_Model\"], errors='ignore', inplace=True)\n",
    "\n",
    "\n",
    "accident_weather_df['NtsbNumber'] = accident_weather_df['NtsbNumber'].str.lower()\n",
    "accident_weather_df['EventDate'] = pd.to_datetime(accident_weather_df['EventDate'], errors='coerce')\n",
    "accident_weather_df['Vehicles.SerialNumber'] = accident_weather_df['Vehicles.SerialNumber'].astype(str).str.lower()\n",
    "accident_weather_df['Vehicles.RegistrationNumber'] = accident_weather_df['Vehicles.RegistrationNumber'].astype(str).str.lower()\n",
    "accident_weather_df['Vehicles.Make'] = accident_weather_df['Vehicles.Make'].astype(str).str.lower()\n",
    "accident_weather_df['Vehicles.Model'] = accident_weather_df['Vehicles.Model'].astype(str).str.lower()\n",
    "\n",
    "accident_weather_df.drop(columns=[\"Vehicles.VehicleNumber\"], errors='ignore', inplace=True)\n",
    "accident_weather_df.rename(columns={\"wx_time\": \"weather_time\"}, inplace=True)\n",
    "\n",
    "for key in accident_weather_df.columns:\n",
    "    if key.startswith('wx_'):\n",
    "        accident_weather_df.rename(columns={key: key[3:]}, inplace=True)\n",
    "\n",
    "# Define the merge keys\n",
    "merge_keys = ['NtsbNumber','EventDate','Vehicles.SerialNumber', 'Vehicles.RegistrationNumber', 'Vehicles.Make', 'Vehicles.Model']\n",
    "\n",
    "# Perform the merge\n",
    "fused_df = accident_weather_df.merge(\n",
    "    matched_results_df,\n",
    "    how='left',\n",
    "    left_on=merge_keys,\n",
    "    right_on=merge_keys\n",
    ")\n",
    "\n",
    "# Drop the duplicate matching columns from the right\n",
    "for key in merge_keys:\n",
    "    fused_df.drop(columns=[f\"{key}_y\"], errors='ignore', inplace=True)\n",
    "    fused_df.rename(columns={f\"{key}_x\": key}, inplace=True)\n",
    "\n",
    "# Save the resulting dataframe\n",
    "fused_df.to_pickle('data_sources/fused/accident_weather_enriched.pkl')\n",
    "fused_df.to_csv(\"data_sources/fused/accident_weather_enriched.csv\", index=False)\n",
    "\n",
    "# Compute matching stats\n",
    "total_records = len(accident_weather_df)\n",
    "matched_records = fused_df['engine_type'].notna().sum()\n",
    "unmatched_records = total_records - matched_records\n",
    "match_percentage = (matched_records / total_records) * 100\n",
    "\n",
    "# Print statistics\n",
    "print(\"Fusion complete. Enriched dataset saved to: data_sources/fused/accident_weather_enriched.pkl\")\n",
    "print(\"\\n--- Matching Statistics ---\")\n",
    "print(f\"Total records in original dataset: {total_records}\")\n",
    "print(f\"Total records matched with binding CSV: {matched_records}\")\n",
    "print(f\"Total unmatched records: {unmatched_records}\")\n",
    "print(f\"Match percentage: {match_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dd24517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23403 entries, 0 to 23402\n",
      "Data columns (total 49 columns):\n",
      " #   Column                        Non-Null Count  Dtype          \n",
      "---  ------                        --------------  -----          \n",
      " 0   Vehicles.DamageLevel          23400 non-null  category       \n",
      " 1   Vehicles.ExplosionType        21880 non-null  category       \n",
      " 2   Vehicles.FireType             23321 non-null  category       \n",
      " 3   Vehicles.SerialNumber         23403 non-null  object         \n",
      " 4   Vehicles.Make                 23403 non-null  object         \n",
      " 5   Vehicles.Model                23403 non-null  object         \n",
      " 6   Vehicles.NumberOfEngines      23403 non-null  int64          \n",
      " 7   Vehicles.RegistrationNumber   23403 non-null  object         \n",
      " 8   Vehicles.FlightOperationType  21593 non-null  object         \n",
      " 9   Vehicles.OperatorName         11290 non-null  object         \n",
      " 10  Oid                           23403 non-null  object         \n",
      " 11  MKey                          23403 non-null  int64          \n",
      " 12  HighestInjury                 23307 non-null  category       \n",
      " 13  NtsbNumber                    23403 non-null  object         \n",
      " 14  ProbableCause                 23205 non-null  object         \n",
      " 15  City                          23403 non-null  object         \n",
      " 16  Country                       23403 non-null  object         \n",
      " 17  EventDate                     23403 non-null  datetime64[ns] \n",
      " 18  State                         23356 non-null  object         \n",
      " 19  Agency                        22495 non-null  object         \n",
      " 20  EventType                     23403 non-null  category       \n",
      " 21  AirportId                     17179 non-null  object         \n",
      " 22  AirportName                   17208 non-null  object         \n",
      " 23  Latitude                      23107 non-null  float64        \n",
      " 24  Longitude                     23106 non-null  float64        \n",
      " 25  TotalInjuryCount              23403 non-null  int64          \n",
      " 26  weather_time                  20858 non-null  datetime64[ns] \n",
      " 27  temperature_2m                20858 non-null  float64        \n",
      " 28  relative_humidity_2m          20858 non-null  float64        \n",
      " 29  dew_point_2m                  20858 non-null  float64        \n",
      " 30  pressure_msl                  20858 non-null  float64        \n",
      " 31  surface_pressure              20858 non-null  float64        \n",
      " 32  precipitation                 20858 non-null  float64        \n",
      " 33  rain                          20858 non-null  float64        \n",
      " 34  snowfall                      20858 non-null  float64        \n",
      " 35  cloud_cover                   20858 non-null  float64        \n",
      " 36  cloud_cover_low               20858 non-null  float64        \n",
      " 37  cloud_cover_mid               20858 non-null  float64        \n",
      " 38  cloud_cover_high              20858 non-null  float64        \n",
      " 39  wind_speed_10m                20858 non-null  float64        \n",
      " 40  wind_speed_100m               20858 non-null  float64        \n",
      " 41  wind_direction_10m            20858 non-null  float64        \n",
      " 42  wind_direction_100m           20858 non-null  float64        \n",
      " 43  wind_gusts_10m                20858 non-null  float64        \n",
      " 44  weather_code                  20858 non-null  float64        \n",
      " 45  snow_depth                    20294 non-null  float64        \n",
      " 46  time_diff                     20858 non-null  timedelta64[ns]\n",
      " 47  engine_count                  4962 non-null   float64        \n",
      " 48  engine_type                   4962 non-null   object         \n",
      "dtypes: category(5), datetime64[ns](2), float64(22), int64(3), object(16), timedelta64[ns](1)\n",
      "memory usage: 8.0+ MB\n"
     ]
    }
   ],
   "source": [
    "fused_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49129579",
   "metadata": {},
   "source": [
    "## fixing issue between `engine_count` and `Vehicles.NumberOfEngines`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2d9efdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion complete. Cleaned dataset saved to:\n",
      "  â€¢ data_sources/fused/accident_weather_final.pkl\n",
      "  â€¢ data_sources/fused/accident_weather_final.csv\n",
      "8 engine count conflicts were resolved by trusting the 'engine_count' value.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df_path = 'data_sources/fused/accident_weather_enriched.pkl'\n",
    "df = pd.read_pickle(df_path)\n",
    "\n",
    "# Convert columns to nullable integers\n",
    "engine_count_int = df['engine_count'].astype('Int64')\n",
    "vehicle_engines = df['Vehicles.NumberOfEngines'].astype('Int64')\n",
    "\n",
    "# Rule 1: Fill NaNs in Vehicles.NumberOfEngines with engine_count\n",
    "df['Vehicles.NumberOfEngines'] = vehicle_engines.combine_first(engine_count_int)\n",
    "\n",
    "# Rule 2: If Vehicles.NumberOfEngines == 0 and engine_count > 0 â†’ trust engine_count\n",
    "mask_replace_zero = (\n",
    "    (df['Vehicles.NumberOfEngines'] == 0) &\n",
    "    (engine_count_int > 0)\n",
    ")\n",
    "df.loc[mask_replace_zero, 'Vehicles.NumberOfEngines'] = engine_count_int[mask_replace_zero]\n",
    "\n",
    "# Rule 3: Overwrite in case of real conflict (â‰  0 and â‰  each other)\n",
    "conflict_mask = (\n",
    "    engine_count_int.notna() &\n",
    "    df['Vehicles.NumberOfEngines'].notna() &\n",
    "    (df['Vehicles.NumberOfEngines'] != engine_count_int) &\n",
    "    (df['Vehicles.NumberOfEngines'] != 0) &\n",
    "    (engine_count_int != 0)\n",
    ")\n",
    "df.loc[conflict_mask, 'Vehicles.NumberOfEngines'] = engine_count_int[conflict_mask]\n",
    "\n",
    "# Drop auxiliary column\n",
    "df.drop(columns=['engine_count'], inplace=True)\n",
    "\n",
    "# Save cleaned and final dataset\n",
    "final_pkl_path = 'data_sources/fused/accident_weather_final.pkl'\n",
    "final_csv_path = 'data_sources/fused/accident_weather_final.csv'\n",
    "\n",
    "df.to_pickle(final_pkl_path)\n",
    "df.to_csv(final_csv_path, index=False)\n",
    "\n",
    "print(f\"Fusion complete. Cleaned dataset saved to:\\n  â€¢ {final_pkl_path}\\n  â€¢ {final_csv_path}\")\n",
    "print(f\"{conflict_mask.sum()} engine count conflicts were resolved by trusting the 'engine_count' value.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
