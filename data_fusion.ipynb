{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd68ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfbad333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vehicles.VehicleNumber</th>\n",
       "      <th>Vehicles.DamageLevel</th>\n",
       "      <th>Vehicles.ExplosionType</th>\n",
       "      <th>Vehicles.FireType</th>\n",
       "      <th>Vehicles.SerialNumber</th>\n",
       "      <th>Vehicles.Make</th>\n",
       "      <th>Vehicles.Model</th>\n",
       "      <th>Vehicles.NumberOfEngines</th>\n",
       "      <th>Vehicles.RegistrationNumber</th>\n",
       "      <th>Vehicles.FlightOperationType</th>\n",
       "      <th>...</th>\n",
       "      <th>Country</th>\n",
       "      <th>EventDate</th>\n",
       "      <th>State</th>\n",
       "      <th>Agency</th>\n",
       "      <th>EventType</th>\n",
       "      <th>AirportId</th>\n",
       "      <th>AirportName</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>TotalInjuryCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15932</th>\n",
       "      <td>1</td>\n",
       "      <td>substantial</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>t18208439</td>\n",
       "      <td>cessna</td>\n",
       "      <td>t182t</td>\n",
       "      <td>1</td>\n",
       "      <td>n357tg</td>\n",
       "      <td>pers</td>\n",
       "      <td>...</td>\n",
       "      <td>usa</td>\n",
       "      <td>2008-06-29 13:40:00</td>\n",
       "      <td>nj</td>\n",
       "      <td>ntsb</td>\n",
       "      <td>acc</td>\n",
       "      <td>n07</td>\n",
       "      <td>lincoln park airport</td>\n",
       "      <td>40.947498</td>\n",
       "      <td>-74.314445</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>1</td>\n",
       "      <td>substantial</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>299</td>\n",
       "      <td>socata</td>\n",
       "      <td>tbm-700 c2</td>\n",
       "      <td>1</td>\n",
       "      <td>n48um</td>\n",
       "      <td>pers</td>\n",
       "      <td>...</td>\n",
       "      <td>usa</td>\n",
       "      <td>2021-07-27 12:05:00</td>\n",
       "      <td>sc</td>\n",
       "      <td>ntsb</td>\n",
       "      <td>acc</td>\n",
       "      <td>lro</td>\n",
       "      <td>mt pleasant rgnl-faison fld</td>\n",
       "      <td>32.903011</td>\n",
       "      <td>-79.784190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11348</th>\n",
       "      <td>1</td>\n",
       "      <td>substantial</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>3290</td>\n",
       "      <td>mooney</td>\n",
       "      <td>m20c</td>\n",
       "      <td>1</td>\n",
       "      <td>n2610w</td>\n",
       "      <td>pers</td>\n",
       "      <td>...</td>\n",
       "      <td>usa</td>\n",
       "      <td>2012-05-19 12:20:00</td>\n",
       "      <td>fl</td>\n",
       "      <td>ntsb</td>\n",
       "      <td>acc</td>\n",
       "      <td>hwo</td>\n",
       "      <td>north perry airport</td>\n",
       "      <td>26.001111</td>\n",
       "      <td>-80.240554</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22156</th>\n",
       "      <td>1</td>\n",
       "      <td>substantial</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>15285671</td>\n",
       "      <td>cessna</td>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "      <td>n94445</td>\n",
       "      <td>inst</td>\n",
       "      <td>...</td>\n",
       "      <td>usa</td>\n",
       "      <td>2003-10-22 11:30:00</td>\n",
       "      <td>fl</td>\n",
       "      <td>ntsb</td>\n",
       "      <td>acc</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>29.230581</td>\n",
       "      <td>-81.459403</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11855</th>\n",
       "      <td>1</td>\n",
       "      <td>minor</td>\n",
       "      <td>none</td>\n",
       "      <td>in-flight</td>\n",
       "      <td>23719</td>\n",
       "      <td>boeing</td>\n",
       "      <td>747-451</td>\n",
       "      <td>4</td>\n",
       "      <td>n661us</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>usa</td>\n",
       "      <td>2011-10-23 16:00:00</td>\n",
       "      <td>mi</td>\n",
       "      <td>ntsb</td>\n",
       "      <td>inc</td>\n",
       "      <td>dtw</td>\n",
       "      <td>detroit international airport</td>\n",
       "      <td>42.349647</td>\n",
       "      <td>-83.059921</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3969</th>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>8118</td>\n",
       "      <td>airbus</td>\n",
       "      <td>a320</td>\n",
       "      <td>2</td>\n",
       "      <td>n328fr</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>usa</td>\n",
       "      <td>2019-08-27 07:50:00</td>\n",
       "      <td>co</td>\n",
       "      <td>ntsb</td>\n",
       "      <td>acc</td>\n",
       "      <td>kden</td>\n",
       "      <td>denver international airport</td>\n",
       "      <td>39.861667</td>\n",
       "      <td>-104.673057</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7118</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>usa</td>\n",
       "      <td>2016-07-17 10:30:00</td>\n",
       "      <td>id</td>\n",
       "      <td>ntsb</td>\n",
       "      <td>acc</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>43.840278</td>\n",
       "      <td>-116.501388</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>1</td>\n",
       "      <td>substantial</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>502-0171</td>\n",
       "      <td>air tractor inc</td>\n",
       "      <td>at-502</td>\n",
       "      <td>1</td>\n",
       "      <td>n1540s</td>\n",
       "      <td>aapl</td>\n",
       "      <td>...</td>\n",
       "      <td>usa</td>\n",
       "      <td>2022-07-21 19:30:00</td>\n",
       "      <td>mn</td>\n",
       "      <td>ntsb</td>\n",
       "      <td>acc</td>\n",
       "      <td>n/a</td>\n",
       "      <td>None</td>\n",
       "      <td>47.930686</td>\n",
       "      <td>-97.012351</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13195</th>\n",
       "      <td>1</td>\n",
       "      <td>substantial</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>1259</td>\n",
       "      <td>dehavilland</td>\n",
       "      <td>beaver dhc-2 mk.1</td>\n",
       "      <td>1</td>\n",
       "      <td>n24150</td>\n",
       "      <td>pers</td>\n",
       "      <td>...</td>\n",
       "      <td>usa</td>\n",
       "      <td>2010-09-19 15:30:00</td>\n",
       "      <td>wa</td>\n",
       "      <td>ntsb</td>\n",
       "      <td>acc</td>\n",
       "      <td>w36</td>\n",
       "      <td>will rogers wiley post</td>\n",
       "      <td>47.518054</td>\n",
       "      <td>-122.218055</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9355</th>\n",
       "      <td>1</td>\n",
       "      <td>minor</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>525b0014</td>\n",
       "      <td>cessna</td>\n",
       "      <td>525</td>\n",
       "      <td>2</td>\n",
       "      <td>n300et</td>\n",
       "      <td>pers</td>\n",
       "      <td>...</td>\n",
       "      <td>usa</td>\n",
       "      <td>2014-04-26 11:30:00</td>\n",
       "      <td>fl</td>\n",
       "      <td>ntsb</td>\n",
       "      <td>inc</td>\n",
       "      <td>7fl6</td>\n",
       "      <td>spruce creek</td>\n",
       "      <td>29.076389</td>\n",
       "      <td>-81.052223</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Vehicles.VehicleNumber Vehicles.DamageLevel Vehicles.ExplosionType  \\\n",
       "15932                       1          substantial                   none   \n",
       "2259                        1          substantial                   none   \n",
       "11348                       1          substantial                   none   \n",
       "22156                       1          substantial                   none   \n",
       "11855                       1                minor                   none   \n",
       "3969                        1                 none                   none   \n",
       "7118                        2                  NaN                unknown   \n",
       "1348                        1          substantial                   none   \n",
       "13195                       1          substantial                   none   \n",
       "9355                        1                minor                   none   \n",
       "\n",
       "      Vehicles.FireType Vehicles.SerialNumber    Vehicles.Make  \\\n",
       "15932              none             t18208439           cessna   \n",
       "2259               none                   299           socata   \n",
       "11348              none                  3290           mooney   \n",
       "22156              none              15285671           cessna   \n",
       "11855         in-flight                 23719           boeing   \n",
       "3969               none                  8118           airbus   \n",
       "7118            unknown               unknown          unknown   \n",
       "1348               none              502-0171  air tractor inc   \n",
       "13195              none                  1259      dehavilland   \n",
       "9355               none              525b0014           cessna   \n",
       "\n",
       "          Vehicles.Model  Vehicles.NumberOfEngines  \\\n",
       "15932              t182t                         1   \n",
       "2259          tbm-700 c2                         1   \n",
       "11348               m20c                         1   \n",
       "22156                152                         1   \n",
       "11855            747-451                         4   \n",
       "3969                a320                         2   \n",
       "7118             unknown                         0   \n",
       "1348              at-502                         1   \n",
       "13195  beaver dhc-2 mk.1                         1   \n",
       "9355                 525                         2   \n",
       "\n",
       "      Vehicles.RegistrationNumber Vehicles.FlightOperationType  ... Country  \\\n",
       "15932                      n357tg                         pers  ...     usa   \n",
       "2259                        n48um                         pers  ...     usa   \n",
       "11348                      n2610w                         pers  ...     usa   \n",
       "22156                      n94445                         inst  ...     usa   \n",
       "11855                      n661us                         None  ...     usa   \n",
       "3969                       n328fr                         None  ...     usa   \n",
       "7118                      unknown                         None  ...     usa   \n",
       "1348                       n1540s                         aapl  ...     usa   \n",
       "13195                      n24150                         pers  ...     usa   \n",
       "9355                       n300et                         pers  ...     usa   \n",
       "\n",
       "                EventDate  State Agency EventType AirportId  \\\n",
       "15932 2008-06-29 13:40:00     nj   ntsb       acc       n07   \n",
       "2259  2021-07-27 12:05:00     sc   ntsb       acc       lro   \n",
       "11348 2012-05-19 12:20:00     fl   ntsb       acc       hwo   \n",
       "22156 2003-10-22 11:30:00     fl   ntsb       acc      None   \n",
       "11855 2011-10-23 16:00:00     mi   ntsb       inc       dtw   \n",
       "3969  2019-08-27 07:50:00     co   ntsb       acc      kden   \n",
       "7118  2016-07-17 10:30:00     id   ntsb       acc      None   \n",
       "1348  2022-07-21 19:30:00     mn   ntsb       acc       n/a   \n",
       "13195 2010-09-19 15:30:00     wa   ntsb       acc       w36   \n",
       "9355  2014-04-26 11:30:00     fl   ntsb       inc      7fl6   \n",
       "\n",
       "                         AirportName   Latitude   Longitude TotalInjuryCount  \n",
       "15932           lincoln park airport  40.947498  -74.314445                0  \n",
       "2259     mt pleasant rgnl-faison fld  32.903011  -79.784190                0  \n",
       "11348            north perry airport  26.001111  -80.240554                1  \n",
       "22156                           None  29.230581  -81.459403                2  \n",
       "11855  detroit international airport  42.349647  -83.059921                0  \n",
       "3969    denver international airport  39.861667 -104.673057                1  \n",
       "7118                            None  43.840278 -116.501388                0  \n",
       "1348                            None  47.930686  -97.012351                0  \n",
       "13195         will rogers wiley post  47.518054 -122.218055                0  \n",
       "9355                    spruce creek  29.076389  -81.052223                0  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"data_sources/filtered/ntsb.pkl\")\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a4057d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_indices count (60%): 14041\n",
      "Number of NaNs assigned: 7020\n",
      "Total NaNs in 'State' column: 7050\n"
     ]
    }
   ],
   "source": [
    "ntsb_og = df.copy()\n",
    "ntsb_copied = df.copy()\n",
    "ntsb_copied = ntsb_copied.rename(columns=\n",
    "    {\"EventDate\": \"Date\", \n",
    "    \"NtsbNumber\": \"ID\", \n",
    "    \"State\": \"Location\"}) # for schema matching \n",
    "\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "\n",
    "n = len(ntsb_copied)\n",
    "\n",
    "# select 60% indices for later use\n",
    "n_forty = int(np.floor(0.6 * n))\n",
    "random_indices = np.random.choice(ntsb_copied.index, n_forty, replace=False)\n",
    "print(f\"random_indices count (60%): {len(random_indices)}\")  # Should be ~0.6 * n\n",
    "\n",
    "# select half of these indices to assign NaN in 'ID'\n",
    "n_missing = int(np.floor(0.5 * n_forty))\n",
    "missing_indices = np.random.choice(random_indices, n_missing, replace=False)\n",
    "\n",
    "# assign NaN only to these missing_indices, in original to then match\n",
    "ntsb_og.loc[missing_indices, \"State\"] = np.nan  # for slot filling\n",
    "\n",
    "print(f\"Number of NaNs assigned: {len(missing_indices)}\")\n",
    "\n",
    "# count total NaNs in 'ID' (including existing NaNs)\n",
    "total_nans = ntsb_og['State'].isna().sum()\n",
    "print(f\"Total NaNs in 'State' column: {total_nans}\")\n",
    "\n",
    "conflict_indices = np.setdiff1d(random_indices, missing_indices)\n",
    "\n",
    "# transform strings for conflict resolution\n",
    "for index, row in ntsb_copied.iterrows():\n",
    "    if index in conflict_indices:\n",
    "        airport = row[\"AirportName\"]\n",
    "        if pd.notna(airport):\n",
    "            result = ' '.join([word[0] + '.' for word in airport.split()])\n",
    "            ntsb_copied.loc[index, \"AirportName\"] = result\n",
    "\n",
    "ntsb_og.to_pickle(\"data_sources/to_fuse/ntsb_og.pkl\")\n",
    "ntsb_copied.to_pickle(\"data_sources/to_fuse/ntsb_copied.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf9ae41",
   "metadata": {},
   "source": [
    "### Fuse NTSB with its dupe for strategy implemenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93bbd76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in 'State' (before fusion): 7050\n",
      "Abbreviated AirportName (before fusion): 5125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging entries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23403/23403 [01:08<00:00, 340.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final fused dataset saved:\n",
      " â€¢ accident_weather_fused_final.pkl\n",
      " â€¢ accident_weather_fused_final.csv\n",
      "Final row count: 23400 (original: 23403, added: 120)\n",
      "NaN in 'State' (after fusion): 47\n",
      "'State' values filled during fusion: 7003 (99.33%)\n",
      "Deduplication removed: 3 rows (0.01%)\n",
      "Abbreviated AirportName (after fusion): 20\n",
      "Resolved AirportName values: 5105 (99.61%)\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "# Load datasets\n",
    "ntsb_og = pd.read_pickle('data_sources/to_fuse/ntsb_og.pkl')\n",
    "ntsb_copied = pd.read_pickle('data_sources/to_fuse/ntsb_copied.pkl')  # adjust if needed\n",
    "\n",
    "# NaN count BEFORE fusion\n",
    "id_nans_before = ntsb_og['State'].isna().sum()\n",
    "print(f\"NaN in 'State' (before fusion): {id_nans_before}\")\n",
    "\n",
    "# Count abbreviated AirportName BEFORE fusion (optional)\n",
    "abbrev_pattern = r'^([a-zA-Z]\\.\\s*)+$'\n",
    "airport_abbrev_before = ntsb_copied['AirportName'].fillna('').str.match(abbrev_pattern).sum()\n",
    "print(f\"Abbreviated AirportName (before fusion): {airport_abbrev_before}\")\n",
    "\n",
    "# Normalize column names in ntsb_copied\n",
    "ntsb_copied = ntsb_copied.rename(columns={\n",
    "    \"Date\": \"EventDate\", \n",
    "    \"ID\": \"NtsbNumber\", \n",
    "    \"Location\": \"State\"\n",
    "})\n",
    "\n",
    "# Drop NtsbNumber\n",
    "ntsb_og = ntsb_og.drop(columns=['NtsbNumber'], errors='ignore')\n",
    "ntsb_copied = ntsb_copied.drop(columns=['NtsbNumber'], errors='ignore')\n",
    "\n",
    "# Helper: fuzzy similarity for airport names\n",
    "def fuzzy_match_airport(row, df_ref):\n",
    "    candidates = df_ref[\n",
    "        (df_ref['EventDate'] == row['EventDate']) &\n",
    "        (df_ref['Vehicles.RegistrationNumber'] == row['Vehicles.RegistrationNumber']) &\n",
    "        (df_ref['Vehicles.SerialNumber'] == row['Vehicles.SerialNumber'])\n",
    "    ]\n",
    "    if candidates.empty or pd.isna(row['AirportName']):\n",
    "        return None\n",
    "    best = candidates['AirportName'].dropna().apply(lambda x: fuzz.partial_ratio(x, row['AirportName']))\n",
    "    if not best.empty and best.max() >= 80:\n",
    "        return candidates.iloc[best.idxmax()]\n",
    "    return None\n",
    "\n",
    "# Merge logic\n",
    "fused_rows = []\n",
    "unmatched_rows = []\n",
    "\n",
    "for _, row in tqdm(ntsb_copied.iterrows(), total=len(ntsb_copied), desc=\"Merging entries\"):\n",
    "    match = ntsb_og[\n",
    "        (ntsb_og['EventDate'] == row['EventDate']) &\n",
    "        (ntsb_og['Vehicles.RegistrationNumber'] == row['Vehicles.RegistrationNumber']) &\n",
    "        (ntsb_og['Vehicles.SerialNumber'] == row['Vehicles.SerialNumber'])\n",
    "    ]\n",
    "\n",
    "    if match.empty:\n",
    "        unmatched_rows.append(row)\n",
    "        continue\n",
    "\n",
    "    merged = match.iloc[0].copy()\n",
    "\n",
    "    # Slot Filling: if merged[col] is NA and row[col] is not, use row[col]\n",
    "    for col in ntsb_copied.columns:\n",
    "        if col in merged.index and pd.isna(merged[col]) and pd.notna(row[col]):\n",
    "            merged[col] = row[col]\n",
    "    \n",
    "    # Conflict resolution: prioritize df unless row[col] is clearly more complete\n",
    "    if pd.notna(row['AirportName']) and pd.notna(merged['AirportName']):\n",
    "        if len(row['AirportName']) > len(merged['AirportName']):  # assume longer name is better\n",
    "            merged['AirportName'] = row['AirportName']\n",
    "\n",
    "    fused_rows.append(merged)\n",
    "\n",
    "# Construct final fused dataframe\n",
    "fused_df = pd.DataFrame(fused_rows)\n",
    "\n",
    "# Reattach unmatched rows (optional)\n",
    "fused_df = pd.concat([fused_df, pd.DataFrame(unmatched_rows)], ignore_index=True)\n",
    "\n",
    "before_dedup = len(fused_df)\n",
    "# Deduplicate\n",
    "fused_df = fused_df.drop_duplicates(subset=[\n",
    "    'EventDate', 'Vehicles.SerialNumber', 'Vehicles.RegistrationNumber'\n",
    "])\n",
    "\n",
    "# Save final fused result\n",
    "fused_df.to_pickle('data_sources/fused/ntsb_fused.pkl')\n",
    "fused_df.to_csv('data_sources/fused/ntsb_fused.csv', index=False)\n",
    "\n",
    "print(\"Final fused dataset saved:\")\n",
    "print(\" â€¢ accident_weather_fused_final.pkl\")\n",
    "print(\" â€¢ accident_weather_fused_final.csv\")\n",
    "print(f\"Final row count: {len(fused_df)} (original: {len(df)}, added: {len(unmatched_rows)})\")\n",
    "\n",
    "# NaN count AFTER fusion\n",
    "id_nans_after = fused_df['State'].isna().sum()\n",
    "print(f\"NaN in 'State' (after fusion): {id_nans_after}\")\n",
    "filled_count = id_nans_before - id_nans_after\n",
    "nan_percent = filled_count / id_nans_before * 100\n",
    "print(f\"'State' values filled during fusion: {filled_count} ({nan_percent:.2f}%)\")\n",
    "\n",
    "# Deduplication stats\n",
    "after_dedup = len(fused_df)\n",
    "dedup_removed = before_dedup - after_dedup\n",
    "dedup_percent = (dedup_removed / before_dedup) * 100\n",
    "print(f\"Deduplication removed: {dedup_removed} rows ({dedup_percent:.2f}%)\")\n",
    "\n",
    "# Count abbreviated AirportName AFTER fusion\n",
    "abbrev_pattern = r'^([a-zA-Z]\\.\\s*)+$'\n",
    "airport_abbrev_after = fused_df['AirportName'].fillna('').str.match(abbrev_pattern).sum()\n",
    "print(f\"Abbreviated AirportName (after fusion): {airport_abbrev_after}\")\n",
    "resolved_count = airport_abbrev_before - airport_abbrev_after\n",
    "resolved_percent = resolved_count / airport_abbrev_before * 100\n",
    "print(f\"Resolved AirportName values: {resolved_count} ({resolved_percent:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f60e6e",
   "metadata": {},
   "source": [
    "## Weather Data Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "295f0cd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m     spatial[\u001b[33m\"\u001b[39m\u001b[33mtime_diff\u001b[39m\u001b[33m\"\u001b[39m] = (spatial[\u001b[33m\"\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m\"\u001b[39m] - acc[\u001b[33m\"\u001b[39m\u001b[33mEventDate\u001b[39m\u001b[33m\"\u001b[39m]).abs()\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# keep the closest hour that is still within MAX_TIME_DIFF\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     spatial = \u001b[43mspatial\u001b[49m\u001b[43m[\u001b[49m\u001b[43mspatial\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtime_diff\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_TIME_DIFF\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     52\u001b[39m     best_rows.append(spatial.nsmallest(\u001b[32m1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtime_diff\u001b[39m\u001b[33m\"\u001b[39m).iloc[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m spatial.empty \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# --- 4. Assemble the fused dataset -------------------------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Errado\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4093\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4091\u001b[39m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[32m   4092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m com.is_bool_indexer(key):\n\u001b[32m-> \u001b[39m\u001b[32m4093\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_bool_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4095\u001b[39m \u001b[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[32m   4096\u001b[39m \u001b[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[32m   4097\u001b[39m is_single_key = \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Errado\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4155\u001b[39m, in \u001b[36mDataFrame._getitem_bool_array\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(deep=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   4154\u001b[39m indexer = key.nonzero()[\u001b[32m0\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m4155\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Errado\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4153\u001b[39m, in \u001b[36mNDFrame._take_with_is_copy\u001b[39m\u001b[34m(self, indices, axis)\u001b[39m\n\u001b[32m   4142\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   4143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis = \u001b[32m0\u001b[39m) -> Self:\n\u001b[32m   4144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4145\u001b[39m \u001b[33;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[32m   4146\u001b[39m \u001b[33;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4151\u001b[39m \u001b[33;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[32m   4152\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4153\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4154\u001b[39m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[32m   4155\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result._get_axis(axis).equals(\u001b[38;5;28mself\u001b[39m._get_axis(axis)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Errado\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4133\u001b[39m, in \u001b[36mNDFrame.take\u001b[39m\u001b[34m(self, indices, axis, **kwargs)\u001b[39m\n\u001b[32m   4128\u001b[39m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[32m   4129\u001b[39m     indices = np.arange(\n\u001b[32m   4130\u001b[39m         indices.start, indices.stop, indices.step, dtype=np.intp\n\u001b[32m   4131\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4133\u001b[39m new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4135\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4137\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes).__finalize__(\n\u001b[32m   4139\u001b[39m     \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mtake\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4140\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Errado\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:894\u001b[39m, in \u001b[36mBaseBlockManager.take\u001b[39m\u001b[34m(self, indexer, axis, verify)\u001b[39m\n\u001b[32m    891\u001b[39m indexer = maybe_convert_indices(indexer, n, verify=verify)\n\u001b[32m    893\u001b[39m new_labels = \u001b[38;5;28mself\u001b[39m.axes[axis].take(indexer)\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Errado\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:704\u001b[39m, in \u001b[36mBaseBlockManager.reindex_indexer\u001b[39m\u001b[34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[39m\n\u001b[32m    701\u001b[39m new_mgr = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(new_blocks, new_axes)\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis == \u001b[32m1\u001b[39m:\n\u001b[32m    703\u001b[39m     \u001b[38;5;66;03m# We can avoid the need to rebuild these\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m704\u001b[39m     new_mgr._blknos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblknos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m     new_mgr._blklocs = \u001b[38;5;28mself\u001b[39m.blklocs.copy()\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_mgr\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# spatial & temporal thresholds\n",
    "LAT_LON_EPS   = 0.10       # â‰ˆ 11 km at mid-latitudes\n",
    "MAX_TIME_DIFF = pd.Timedelta('3h')   # reject candidates > 3 h away\n",
    "\n",
    "# --- 1. Load data -------------------------------------------------------------\n",
    "ntsb_path    = Path(\"data_sources/filtered/ntsb.pkl\")\n",
    "weather_path = Path(\"data_sources/filtered/weather.pkl\")\n",
    "\n",
    "ntsb    = pd.read_pickle(ntsb_path)\n",
    "weather = pd.read_pickle(weather_path)\n",
    "\n",
    "# ensure correct dtypes\n",
    "ntsb[\"EventDate\"] = pd.to_datetime(ntsb[\"EventDate\"], errors=\"coerce\")\n",
    "weather[\"time\"]   = pd.to_datetime(weather[\"time\"],   errors=\"coerce\")\n",
    "\n",
    "# --- 2. Blocking on event *date* ---------------------------------------------\n",
    "ntsb[\"event_day\"]    = ntsb[\"EventDate\"].dt.date\n",
    "weather[\"weather_day\"] = weather[\"time\"].dt.date\n",
    "\n",
    "weather_by_day = {d: w.reset_index(drop=True)\n",
    "                  for d, w in weather.groupby(\"weather_day\")}\n",
    "\n",
    "# --- 3. Similarity matching & temporal precedence -----------------------------\n",
    "best_rows = []       # stores best-matching weather rows (or None)\n",
    "\n",
    "for _, acc in ntsb.iterrows():\n",
    "    day_candidates = weather_by_day.get(acc[\"event_day\"], pd.DataFrame())\n",
    "    if day_candidates.empty:\n",
    "        best_rows.append(None); continue\n",
    "    \n",
    "    # coarse spatial filter  |lat/lon diff| < LAT_LON_EPS\n",
    "    spatial = day_candidates[\n",
    "        (day_candidates[\"time\"].notna()) &\n",
    "        (day_candidates[\"AccidentID\"].notna()) &       # keeps malformed rows out\n",
    "        (day_candidates[\"AccidentID\"].str.contains('_'))  # quick sanity\n",
    "    ].copy()\n",
    "\n",
    "    spatial = spatial[\n",
    "        (np.abs(spatial[\"AccidentID\"].str.split('_').str[-2].astype(float) - acc[\"Latitude\" ] ) < LAT_LON_EPS) &\n",
    "        (np.abs(spatial[\"AccidentID\"].str.split('_').str[-1].astype(float) - acc[\"Longitude\"]) < LAT_LON_EPS)\n",
    "    ]\n",
    "\n",
    "    if spatial.empty:\n",
    "        best_rows.append(None); continue\n",
    "    \n",
    "    # temporal distance to the accident moment\n",
    "    spatial[\"time_diff\"] = (spatial[\"time\"] - acc[\"EventDate\"]).abs()\n",
    "    \n",
    "    # keep the closest hour that is still within MAX_TIME_DIFF\n",
    "    spatial = spatial[spatial[\"time_diff\"] <= MAX_TIME_DIFF]\n",
    "    \n",
    "    best_rows.append(spatial.nsmallest(1, \"time_diff\").iloc[0] if not spatial.empty else None)\n",
    "\n",
    "# --- 4. Assemble the fused dataset -------------------------------------------\n",
    "weather_match_df = pd.DataFrame.from_records(\n",
    "    [row.to_dict() if row is not None else {}          # convert None into an empty dict {}\n",
    "     for row in best_rows],\n",
    "    index=ntsb.index                                   # keeps row-alignment\n",
    ")\n",
    "\n",
    "accident_weather = pd.concat(\n",
    "    [ntsb.reset_index(drop=True),\n",
    "     weather_match_df.add_prefix(\"wx_\")],              # prefix to avoid clashes\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# --- 5. Quick diagnostics -----------------------------------------------------\n",
    "total_accidents = len(ntsb)\n",
    "matched         = accident_weather[\"wx_time\"].notna().sum()\n",
    "print(f\"Matched {matched} of {total_accidents} accidents \"\n",
    "      f\"({matched / total_accidents:.1%})\")\n",
    "\n",
    "if matched:\n",
    "    print(\"\\nTime difference (min) for matched rows:\")\n",
    "    print((accident_weather.loc[accident_weather.wx_time.notna(), \"wx_time_diff\"]\n",
    "           .dt.total_seconds().div(60)\n",
    "           .describe().round(2)))\n",
    "\n",
    "    print(\"\\nSpatial deltas (deg lat/lon) for matched rows:\")\n",
    "    lat_delta = np.abs(accident_weather[\"Latitude\"] - accident_weather[\"wx_AccidentID\"]\n",
    "                       .str.split('_').str[-2].astype(float))\n",
    "    lon_delta = np.abs(accident_weather[\"Longitude\"] - accident_weather[\"wx_AccidentID\"]\n",
    "                       .str.split('_').str[-1].astype(float))\n",
    "    print(pd.concat({\"lat\": lat_delta, \"lon\": lon_delta}, axis=1).describe().round(4))\n",
    "\n",
    "accident_weather.drop(columns=[\"event_day\",\"wx_AccidentID\",\"wx_weather_day\"], errors='ignore', inplace=True)\n",
    "accident_weather.to_pickle(\"data_sources/fused/accident_weather.pkl\")\n",
    "accident_weather.to_csv(\"data_sources/fused/accident_weather.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c4e03a",
   "metadata": {},
   "source": [
    "## Matched Aircraft Data Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f1fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion complete. Enriched dataset saved to: data_sources/fused/accident_weather_enriched.pkl\n",
      "\n",
      "--- Matching Statistics ---\n",
      "Total records in original dataset: 23403\n",
      "Total records matched with binding CSV: 4962\n",
      "Total unmatched records: 18441\n",
      "Match percentage: 21.20%\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "accident_weather_path = 'data_sources/fused/accident_weather.pkl'\n",
    "matched_results_path = 'data_sources/binding/matched_results.csv'\n",
    "\n",
    "accident_weather_df = pd.read_pickle(accident_weather_path)\n",
    "matched_results_df = pd.read_csv(matched_results_path)\n",
    "\n",
    "\n",
    "def clean_text(s):\n",
    "    \"\"\" Normalizzazione del testo: rimozione di caratteri speciali, lowercase e spazi extra. \"\"\"\n",
    "    return re.sub(r'\\W+', ' ', str(s)).lower().strip()\n",
    "\n",
    "# Pulizia dei dati\n",
    "accident_weather_df['Vehicles.Model'] = accident_weather_df['Vehicles.Model'].apply(clean_text)\n",
    "accident_weather_df['Vehicles.Make'] = accident_weather_df['Vehicles.Make'].apply(clean_text)\n",
    "\n",
    "# Normalize casing for matching\n",
    "matched_results_df['NtsbNumber'] = matched_results_df['NtsbNumber'].str.lower()\n",
    "matched_results_df['EventDate'] = pd.to_datetime(matched_results_df['EventDate'], errors='coerce')\n",
    "matched_results_df['Vehicles.SerialNumber'] = matched_results_df['Vehicles.SerialNumber'].str.lower()\n",
    "matched_results_df['Vehicles.RegistrationNumber'] = matched_results_df['Vehicles.RegistrationNumber'].str.lower()\n",
    "matched_results_df['Vehicles.Make'] = matched_results_df['Vehicles.Make'].str.lower()\n",
    "matched_results_df['Vehicles.Model'] = matched_results_df['Vehicles.Model'].str.lower()\n",
    "\n",
    "matched_results_df.drop(columns=[\"JW_Score\",\"LEV_Score\",\"Jac_Score\",\"SimilarityScore\",\"Matched_Aircraft_Model\"], errors='ignore', inplace=True)\n",
    "\n",
    "\n",
    "accident_weather_df['NtsbNumber'] = accident_weather_df['NtsbNumber'].str.lower()\n",
    "accident_weather_df['EventDate'] = pd.to_datetime(accident_weather_df['EventDate'], errors='coerce')\n",
    "accident_weather_df['Vehicles.SerialNumber'] = accident_weather_df['Vehicles.SerialNumber'].astype(str).str.lower()\n",
    "accident_weather_df['Vehicles.RegistrationNumber'] = accident_weather_df['Vehicles.RegistrationNumber'].astype(str).str.lower()\n",
    "accident_weather_df['Vehicles.Make'] = accident_weather_df['Vehicles.Make'].astype(str).str.lower()\n",
    "accident_weather_df['Vehicles.Model'] = accident_weather_df['Vehicles.Model'].astype(str).str.lower()\n",
    "\n",
    "accident_weather_df.drop(columns=[\"Vehicles.VehicleNumber\"], errors='ignore', inplace=True)\n",
    "accident_weather_df.rename(columns={\"wx_time\": \"weather_time\"}, inplace=True)\n",
    "\n",
    "for key in accident_weather_df.columns:\n",
    "    if key.startswith('wx_'):\n",
    "        accident_weather_df.rename(columns={key: key[3:]}, inplace=True)\n",
    "\n",
    "# Define the merge keys\n",
    "merge_keys = ['NtsbNumber','EventDate','Vehicles.SerialNumber', 'Vehicles.RegistrationNumber', 'Vehicles.Make', 'Vehicles.Model']\n",
    "\n",
    "# Perform the merge\n",
    "fused_df = accident_weather_df.merge(\n",
    "    matched_results_df,\n",
    "    how='left',\n",
    "    left_on=merge_keys,\n",
    "    right_on=merge_keys\n",
    ")\n",
    "\n",
    "# Drop the duplicate matching columns from the right\n",
    "for key in merge_keys:\n",
    "    fused_df.drop(columns=[f\"{key}_y\"], errors='ignore', inplace=True)\n",
    "    fused_df.rename(columns={f\"{key}_x\": key}, inplace=True)\n",
    "\n",
    "# Save the resulting dataframe\n",
    "fused_df.to_pickle('data_sources/fused/accident_weather_enriched.pkl')\n",
    "fused_df.to_csv(\"data_sources/fused/accident_weather_enriched.csv\", index=False)\n",
    "\n",
    "# Compute matching stats\n",
    "total_records = len(accident_weather_df)\n",
    "matched_records = fused_df['Matched_Aircraft_Model'].notna().sum()\n",
    "unmatched_records = total_records - matched_records\n",
    "match_percentage = (matched_records / total_records) * 100\n",
    "\n",
    "# Print statistics\n",
    "print(\"Fusion complete. Enriched dataset saved to: data_sources/fused/accident_weather_enriched.pkl\")\n",
    "print(\"\\n--- Matching Statistics ---\")\n",
    "print(f\"Total records in original dataset: {total_records}\")\n",
    "print(f\"Total records matched with binding CSV: {matched_records}\")\n",
    "print(f\"Total unmatched records: {unmatched_records}\")\n",
    "print(f\"Match percentage: {match_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd24517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23403 entries, 0 to 23402\n",
      "Data columns (total 49 columns):\n",
      " #   Column                        Non-Null Count  Dtype          \n",
      "---  ------                        --------------  -----          \n",
      " 0   Vehicles.DamageLevel          23400 non-null  category       \n",
      " 1   Vehicles.ExplosionType        21880 non-null  category       \n",
      " 2   Vehicles.FireType             23321 non-null  category       \n",
      " 3   Vehicles.SerialNumber         23403 non-null  object         \n",
      " 4   Vehicles.Make                 23403 non-null  object         \n",
      " 5   Vehicles.Model                23403 non-null  object         \n",
      " 6   Vehicles.NumberOfEngines      23403 non-null  int64          \n",
      " 7   Vehicles.RegistrationNumber   23403 non-null  object         \n",
      " 8   Vehicles.FlightOperationType  21593 non-null  object         \n",
      " 9   Vehicles.OperatorName         11290 non-null  object         \n",
      " 10  Oid                           23403 non-null  object         \n",
      " 11  MKey                          23403 non-null  int64          \n",
      " 12  HighestInjury                 23307 non-null  category       \n",
      " 13  NtsbNumber                    23403 non-null  object         \n",
      " 14  ProbableCause                 23205 non-null  object         \n",
      " 15  City                          23403 non-null  object         \n",
      " 16  Country                       23403 non-null  object         \n",
      " 17  EventDate                     23403 non-null  datetime64[ns] \n",
      " 18  State                         23356 non-null  object         \n",
      " 19  Agency                        22495 non-null  object         \n",
      " 20  EventType                     23403 non-null  category       \n",
      " 21  AirportId                     17179 non-null  object         \n",
      " 22  AirportName                   17208 non-null  object         \n",
      " 23  Latitude                      23107 non-null  float64        \n",
      " 24  Longitude                     23106 non-null  float64        \n",
      " 25  TotalInjuryCount              23403 non-null  int64          \n",
      " 26  weather_time                  20858 non-null  datetime64[ns] \n",
      " 27  temperature_2m                20858 non-null  float64        \n",
      " 28  relative_humidity_2m          20858 non-null  float64        \n",
      " 29  dew_point_2m                  20858 non-null  float64        \n",
      " 30  pressure_msl                  20858 non-null  float64        \n",
      " 31  surface_pressure              20858 non-null  float64        \n",
      " 32  precipitation                 20858 non-null  float64        \n",
      " 33  rain                          20858 non-null  float64        \n",
      " 34  snowfall                      20858 non-null  float64        \n",
      " 35  cloud_cover                   20858 non-null  float64        \n",
      " 36  cloud_cover_low               20858 non-null  float64        \n",
      " 37  cloud_cover_mid               20858 non-null  float64        \n",
      " 38  cloud_cover_high              20858 non-null  float64        \n",
      " 39  wind_speed_10m                20858 non-null  float64        \n",
      " 40  wind_speed_100m               20858 non-null  float64        \n",
      " 41  wind_direction_10m            20858 non-null  float64        \n",
      " 42  wind_direction_100m           20858 non-null  float64        \n",
      " 43  wind_gusts_10m                20858 non-null  float64        \n",
      " 44  weather_code                  20858 non-null  float64        \n",
      " 45  snow_depth                    20294 non-null  float64        \n",
      " 46  time_diff                     20858 non-null  timedelta64[ns]\n",
      " 47  engine_count                  4962 non-null   float64        \n",
      " 48  engine_type                   4962 non-null   object         \n",
      "dtypes: category(5), datetime64[ns](2), float64(22), int64(3), object(16), timedelta64[ns](1)\n",
      "memory usage: 8.0+ MB\n"
     ]
    }
   ],
   "source": [
    "fused_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49129579",
   "metadata": {},
   "source": [
    "## fixing issue between `engine_count` and `Vehicles.NumberOfEngines`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d9efdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fusion complete. Cleaned dataset saved to:\n",
      "  â€¢ data_sources/fused/accident_weather_final.pkl\n",
      "  â€¢ data_sources/fused/accident_weather_final.csv\n",
      "ðŸ”„ 8 engine count conflicts were resolved by trusting the 'engine_count' value.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df_path = 'data_sources/fused/accident_weather_enriched.pkl'\n",
    "df = pd.read_pickle(df_path)\n",
    "\n",
    "# Convert columns to nullable integers\n",
    "engine_count_int = df['engine_count'].astype('Int64')\n",
    "vehicle_engines = df['Vehicles.NumberOfEngines'].astype('Int64')\n",
    "\n",
    "# Rule 1: Fill NaNs in Vehicles.NumberOfEngines with engine_count\n",
    "df['Vehicles.NumberOfEngines'] = vehicle_engines.combine_first(engine_count_int)\n",
    "\n",
    "# Rule 2: If Vehicles.NumberOfEngines == 0 and engine_count > 0 â†’ trust engine_count\n",
    "mask_replace_zero = (\n",
    "    (df['Vehicles.NumberOfEngines'] == 0) &\n",
    "    (engine_count_int > 0)\n",
    ")\n",
    "df.loc[mask_replace_zero, 'Vehicles.NumberOfEngines'] = engine_count_int[mask_replace_zero]\n",
    "\n",
    "# Rule 3: Overwrite in case of real conflict (â‰  0 and â‰  each other)\n",
    "conflict_mask = (\n",
    "    engine_count_int.notna() &\n",
    "    df['Vehicles.NumberOfEngines'].notna() &\n",
    "    (df['Vehicles.NumberOfEngines'] != engine_count_int) &\n",
    "    (df['Vehicles.NumberOfEngines'] != 0) &\n",
    "    (engine_count_int != 0)\n",
    ")\n",
    "df.loc[conflict_mask, 'Vehicles.NumberOfEngines'] = engine_count_int[conflict_mask]\n",
    "\n",
    "# Drop auxiliary column\n",
    "df.drop(columns=['engine_count'], inplace=True)\n",
    "\n",
    "# Save cleaned and final dataset\n",
    "final_pkl_path = 'data_sources/fused/accident_weather_final.pkl'\n",
    "final_csv_path = 'data_sources/fused/accident_weather_final.csv'\n",
    "\n",
    "df.to_pickle(final_pkl_path)\n",
    "df.to_csv(final_csv_path, index=False)\n",
    "\n",
    "print(f\"âœ… Fusion complete. Cleaned dataset saved to:\\n  â€¢ {final_pkl_path}\\n  â€¢ {final_csv_path}\")\n",
    "print(f\"ðŸ”„ {conflict_mask.sum()} engine count conflicts were resolved by trusting the 'engine_count' value.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
