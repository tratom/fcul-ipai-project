{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Phase 1 - Aviation Accident Data Integration\n",
    "### Group 03:\n",
    "- Tommaso Tragno - fc64699\n",
    "- Manuel Cardoso - fc56274\n",
    "- Chen Cheng - fc64872\n",
    "- Cristian Tedesco - fc65149"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "import time\n",
    "import seaborn as sns\n",
    "import calendar\n",
    "#import py_stringmatching as sm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data_sources/'\n",
    "FILTERED_PATH = 'filtered/'\n",
    "\n",
    "NTSB_DATA = 'ntsb-us-2003-2023.json'\n",
    "AIR_TRAFFIC_DATA = 'u-s-airline-traffic-data.csv'\n",
    "AIRCRAFT_DATA = 'aircraft_data.csv' #'aircraft_data_cleaned.csv' # the \"cleaned\" one contains the data cleaning part\n",
    "WEATHER_DATA = 'weather_results.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load NTSB JSON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH+NTSB_DATA, 'r', encoding='utf-8') as f:\n",
    "    ntsb_raw_data = json.load(f)\n",
    "\n",
    "# Each record is one accident/incident entry in a list\n",
    "print(f'\\n--- NTSB JSON loaded: {len(ntsb_raw_data)} total records found ---')\n",
    "\n",
    "# Convert to a DataFrame (this will flatten top-level fields)\n",
    "# For nested fields like 'Vehicles', we might do a separate flatten later\n",
    "df_ntsb = pd.json_normalize(ntsb_raw_data, \n",
    "                            meta=[\n",
    "                                'Oid','MKey','Closed','CompletionStatus','HasSafetyRec',\n",
    "                                'HighestInjury','IsStudy','Mode','NtsbNumber',\n",
    "                                'OriginalPublishedDate','MostRecentReportType','ProbableCause',\n",
    "                                'City','Country','EventDate','State','Agency','BoardLaunch',\n",
    "                                'BoardMeetingDate','DocketDate','EventType','Launch','ReportDate',\n",
    "                                'ReportNum','ReportType','AirportId','AirportName','AnalysisNarrative',\n",
    "                                'FactualNarrative','PrelimNarrative','FatalInjuryCount','MinorInjuryCount',\n",
    "                                'SeriousInjuryCount','InvestigationClass','AccidentSiteCondition',\n",
    "                                'Latitude','Longitude','DocketOriginalPublishDate'\n",
    "                            ],\n",
    "                            record_path=['Vehicles'],  # This flattens out the 'Vehicles' array\n",
    "                            record_prefix='Vehicles.'\n",
    "                           )\n",
    "\n",
    "print('\\n--- Flattened NTSB DataFrame (including Vehicles info): ---')\n",
    "\n",
    "# print(df_ntsb.info())\n",
    "\n",
    "# combines all injury counts to 1 column\n",
    "df_ntsb['TotalInjuryCount'] = df_ntsb[['FatalInjuryCount', 'MinorInjuryCount', 'SeriousInjuryCount']].sum(axis=1)\n",
    "\n",
    "# dropping unnecessary columns\n",
    "df_ntsb.drop(columns=['AnalysisNarrative','FactualNarrative','PrelimNarrative','InvestigationClass','BoardLaunch'\n",
    "                      ,'BoardMeetingDate','Launch','IsStudy','OriginalPublishedDate','DocketOriginalPublishDate'\n",
    "                      ,'ReportType','ReportNum','ReportDate','MostRecentReportType','FatalInjuryCount','MinorInjuryCount'\n",
    "                      ,'SeriousInjuryCount','DocketDate','Mode','HasSafetyRec','CompletionStatus','Closed'\n",
    "                      ,'Vehicles.AircraftCategory','Vehicles.AmateurBuilt','Vehicles.EventID','Vehicles.AirMedical'\n",
    "                      ,'Vehicles.AirMedicalType','Vehicles.flightScheduledType','Vehicles.flightServiceType'\n",
    "                      ,'Vehicles.flightTerminalType','Vehicles.RegisteredOwner','Vehicles.RegulationFlightConductedUnder'\n",
    "                      ,'Vehicles.RepGenFlag','Vehicles.RevenueSightseeing','Vehicles.SecondPilotPresent','Vehicles.Damage'\n",
    "                      ,'AccidentSiteCondition'], inplace=True) \n",
    "\n",
    "# dropping NaT entries from EventDate\n",
    "df_ntsb = df_ntsb.dropna(subset=['EventDate'])\n",
    "\n",
    "# Type Conversion\n",
    "df_ntsb['EventDate'] = pd.to_datetime(df_ntsb['EventDate']).dt.tz_localize(None)\n",
    "df_ntsb['Vehicles.VehicleNumber'] = pd.to_numeric(df_ntsb['Vehicles.VehicleNumber'], errors='coerce').astype(int)\n",
    "df_ntsb['MKey'] = pd.to_numeric(df_ntsb['MKey'], errors='coerce').astype(int)\n",
    "df_ntsb['Vehicles.NumberOfEngines'] = pd.to_numeric(df_ntsb['Vehicles.NumberOfEngines'], errors='coerce').fillna(0).astype(int)\n",
    "df_ntsb['Latitude'] = pd.to_numeric(df_ntsb['Latitude'], errors='coerce').astype(float)\n",
    "df_ntsb['Longitude'] = pd.to_numeric(df_ntsb['Longitude'], errors='coerce').astype(float)\n",
    "df_ntsb['TotalInjuryCount'] = pd.to_numeric(df_ntsb['TotalInjuryCount'], errors='coerce').astype(int)\n",
    "\n",
    "categorical_cols = [\n",
    "    'Vehicles.DamageLevel',\n",
    "    'Vehicles.ExplosionType',\n",
    "    'Vehicles.FireType',\n",
    "    'HighestInjury',\n",
    "    'EventType',\n",
    "    'AccidentSiteCondition'\n",
    "]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df_ntsb.columns:\n",
    "        df_ntsb[col] = df_ntsb[col].astype('category')\n",
    "\n",
    "df_ntsb = df_ntsb.map(lambda x: x.lower() if isinstance(x, str) else x) # make all appropriate values lowercase\n",
    "\n",
    "print(df_ntsb.info())\n",
    "\n",
    "print('\\n--- Saving filtered NTSB DataFrame... ---')\n",
    "df_ntsb.to_pickle(PATH+'filtered/ntsb.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Weather JSON Data\n",
    "(after fetching the data from open-meteo API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH+WEATHER_DATA, 'r', encoding='utf-8') as f:\n",
    "    weather_raw_data = json.load(f)\n",
    "\n",
    "# Each record is one day weather entry in a list\n",
    "print(f'\\n--- Weather JSON loaded: {len(weather_raw_data)} total records found ---')\n",
    "\n",
    "# weather_data is a dict, e.g.:\n",
    "# {\n",
    "#   \"cen24la079_2023-12-31_41.610278_-90.588361\": {\n",
    "#       \"time\": [...],\n",
    "#       \"temperature_2m\": [...],\n",
    "#       ...\n",
    "#   }\n",
    "# }\n",
    "\n",
    "# Flatten into a tabular structure\n",
    "all_rows = []\n",
    "num_skip = 0\n",
    "\n",
    "for accident_id, subdict in weather_raw_data.items():\n",
    "    # subdict is a dict with keys like \"time\", \"temperature_2m\", ...\n",
    "    # Each key is an array of the same length (24 hours).\n",
    "    times = subdict.get(\"time\", None)\n",
    "    if times is None:\n",
    "        print(f'Skipping {accident_id}: no \"time\" found.')\n",
    "        num_skip += 1\n",
    "        continue\n",
    "    num_hours = len(subdict[\"time\"])\n",
    "    for i in range(num_hours):\n",
    "        row = {\"AccidentID\": accident_id}  # store the top-level key\n",
    "        for param, values_array in subdict.items():\n",
    "            # param: \"time\", \"temperature_2m\", ...\n",
    "            row[param] = values_array[i]  # pick the ith hourâ€™s value\n",
    "        all_rows.append(row)\n",
    "\n",
    "df_weather = pd.DataFrame(all_rows)\n",
    "\n",
    "# The missing values exists because not all accident have position data\n",
    "# this cause the api to return empty data.\n",
    "print(\"Skipped {} records over {} accidents.\".format(num_skip, len(weather_raw_data.items())))\n",
    "\n",
    "# Type conversion\n",
    "df_weather[\"time\"] = pd.to_datetime(df_weather[\"time\"], errors=\"coerce\")\n",
    "\n",
    "int_columns = [\n",
    "    \"relative_humidity_2m\",\n",
    "    \"cloud_cover_low\",\n",
    "    \"cloud_cover_mid\",\n",
    "    \"cloud_cover_high\",\n",
    "    \"wind_direction_10m\",\n",
    "    \"wind_direction_100m\",\n",
    "    \"weather_code\"\n",
    "]\n",
    "float_columns = [\n",
    "    \"temperature_2m\",\n",
    "    \"dew_point_2m\",\n",
    "    \"pressure_msl\",\n",
    "    \"surface_pressure\",\n",
    "    \"precipitation\",\n",
    "    \"rain\",\n",
    "    \"snowfall\",\n",
    "    \"wind_speed_10m\",\n",
    "    \"wind_speed_100m\",\n",
    "    \"wind_gusts_10m\",\n",
    "    \"snow_depth\"\n",
    "]\n",
    "for col in int_columns:\n",
    "    df_weather[col] = pd.to_numeric(df_weather[col], errors=\"coerce\").astype(int)\n",
    "for col in float_columns:\n",
    "    df_weather[col] = pd.to_numeric(df_weather[col], errors=\"coerce\").astype(float)\n",
    "\n",
    "\n",
    "print(\"\\n--- Weather DataFrame sample ---\")\n",
    "print(df_weather.info())\n",
    "\n",
    "print('\\n--- Saving filtered Weather DataFrame... ---')\n",
    "df_weather.to_pickle(PATH+'filtered/weather.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Airline Traffic CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airline_traffic = pd.read_csv(PATH+AIR_TRAFFIC_DATA, encoding='utf-8')\n",
    "\n",
    "print(f'\\n--- Airline CSV loaded: {df_airline_traffic.shape[0]} rows, {df_airline_traffic.shape[1]} columns ---')\n",
    "\n",
    "# dropping unnecessary columns\n",
    "df_airline_traffic.drop(columns=['Dom_RPM','Int_RPM','RPM','Dom_ASM','Int_ASM','ASM'], inplace=True) \n",
    "\n",
    "# print(df_airline_traffic.info())\n",
    "\n",
    "# Remove commas from all columns and then convert\n",
    "df_airline_traffic = df_airline_traffic.replace(',', '', regex=True)\n",
    "\n",
    "# Now convert each column to numeric. If everything converts well, no rows become NaN.\n",
    "df_airline_traffic = df_airline_traffic.apply(pd.to_numeric, errors='coerce').astype(int)\n",
    "\n",
    "print(df_airline_traffic.info())\n",
    "\n",
    "print('\\n--- Saving filtered Airline DataFrame... ---')\n",
    "df_airline_traffic.to_pickle(PATH+'filtered/airline.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Aircraft CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aircraft = pd.read_csv(PATH+AIRCRAFT_DATA, encoding='utf-8')\n",
    "\n",
    "print(f'\\n--- Aircraft CSV loaded: {df_aircraft.shape[0]} rows, {df_aircraft.shape[1]} columns ---')\n",
    "\n",
    "# print(df_aircraft.info())\n",
    "\n",
    "# dropping unnecessary columns\n",
    "df_aircraft.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df_aircraft.drop(columns=['retired'], inplace=True)\n",
    "\n",
    "# make string values lowercase\n",
    "df_aircraft['aircraft'] = df_aircraft['aircraft'].str.lower()\n",
    "\n",
    "# Type Conversion\n",
    "df_aircraft['nbBuilt'] = pd.to_numeric(df_aircraft['nbBuilt'], errors='coerce').astype(int)\n",
    "df_aircraft['startDate'] = pd.to_numeric(df_aircraft['startDate'], errors='coerce').astype(int)\n",
    "df_aircraft['endDate'] = pd.to_numeric(df_aircraft['endDate'], errors='coerce').astype('Int64')  # Use 'Int64' for nullable integers\n",
    "\n",
    "print(df_aircraft.info())\n",
    "\n",
    "print('\\n--- Saving filtered Aircraft DataFrame... ---')\n",
    "df_aircraft.to_pickle(PATH+'filtered/aircraft.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_dataframe(df, name='DataFrame'):\n",
    "    print(f'\\n=== Profiling {name} ===')\n",
    "    print(f'Total Rows: {len(df)}')\n",
    "    print(f'Total Columns: {len(df.columns)}\\n')\n",
    "    \n",
    "    profile_results = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        series = df[col]\n",
    "        col_dtype = series.dtype\n",
    "        \n",
    "        # Basic counts\n",
    "        total_count = len(series)\n",
    "        missing_vals = series.isna().sum()\n",
    "        non_null_count = total_count - missing_vals\n",
    "        missing_perc = (missing_vals / total_count) * 100\n",
    "        unique_vals = series.nunique(dropna=False)\n",
    "        \n",
    "        # Mode & frequency\n",
    "        try:\n",
    "            modes = series.mode(dropna=True)\n",
    "            mode_val = modes.iloc[0] if len(modes) > 0 else np.nan\n",
    "            mode_freq = (series == mode_val).sum(skipna=True)\n",
    "        except:\n",
    "            mode_val, mode_freq = np.nan, np.nan\n",
    "        \n",
    "        # Initialize placeholders\n",
    "        mean_ = np.nan\n",
    "        min_  = np.nan\n",
    "        q25   = np.nan\n",
    "        q50   = np.nan\n",
    "        q75   = np.nan\n",
    "        max_  = np.nan\n",
    "        std_  = np.nan  # only for numeric columns\n",
    "\n",
    "        # Numeric columns\n",
    "        if pd.api.types.is_numeric_dtype(series):\n",
    "            mean_ = series.mean(skipna=True)\n",
    "            min_  = series.min(skipna=True)\n",
    "            q25   = series.quantile(0.25)\n",
    "            q50   = series.quantile(0.50)\n",
    "            q75   = series.quantile(0.75)\n",
    "            max_  = series.max(skipna=True)\n",
    "            std_  = series.std(skipna=True)\n",
    "\n",
    "        # Datetime columns\n",
    "        elif pd.api.types.is_datetime64_any_dtype(series):\n",
    "            # We can compute mean & quartiles by time. \n",
    "            # .quantile() and .mean() are valid for datetime in pandas\n",
    "            # They return a Timestamp for mean, \n",
    "            # and Timestamps for quantiles\n",
    "            if non_null_count > 0:\n",
    "                mean_ = series.mean(skipna=True)\n",
    "                min_  = series.min(skipna=True)\n",
    "                q25   = series.quantile(0.25)\n",
    "                q50   = series.quantile(0.50)\n",
    "                q75   = series.quantile(0.75)\n",
    "                max_  = series.max(skipna=True)\n",
    "            # We skip std_ for datetime.\n",
    "\n",
    "        # Categorical/object columns \n",
    "        # do not get numeric stats (we keep them as NaN).\n",
    "\n",
    "        profile_results.append((\n",
    "            col,\n",
    "            str(col_dtype),\n",
    "            total_count,\n",
    "            non_null_count,\n",
    "            missing_vals,\n",
    "            round(missing_perc, 2),\n",
    "            unique_vals,\n",
    "            mode_val,\n",
    "            mode_freq,\n",
    "            mean_,\n",
    "            min_,\n",
    "            q25,\n",
    "            q50,\n",
    "            q75,\n",
    "            max_,\n",
    "            std_\n",
    "        ))\n",
    "\n",
    "    columns = [\n",
    "        'Column', 'DataType', 'TotalCount', 'NonNullCount', 'NumMissing',\n",
    "        'MissingPerc', 'Cardinality', 'Mode', 'ModeFreq',\n",
    "        'Mean', 'Min', 'Q25', 'Q50', 'Q75', 'Max', 'Std'\n",
    "    ]\n",
    "\n",
    "    prof_df = pd.DataFrame(profile_results, columns=columns)\n",
    "    \n",
    "    return prof_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NTSB Data Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntsb_profile = profile_dataframe(df_ntsb, name='NTSB Data')\n",
    "display(HTML(ntsb_profile.to_html()))\n",
    "ntsb_profile.to_csv(PATH+'profiling/ntsb_profile.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights from the data profile results:\n",
    "\n",
    "- there are some `null` values for Latitude and Longitude --> we keep like this, but they should be handled during the API calls to open-meteo\n",
    "- there are less unique `NtsbNumber` than rows --> for incident where more than one aircraft is involved, the rows are duplicated with different values for Vehicles characteristic, and same value for incident data (look at the following example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ntsb.loc[df_ntsb['NtsbNumber']=='ops24la011']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weather Data Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_profile = profile_dataframe(df_weather, name='Weather Data')\n",
    "display(HTML(weather_profile.to_html()))\n",
    "weather_profile.to_csv(PATH+'profiling/weather_profile.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Air Traffic Data Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_profile = profile_dataframe(df_airline_traffic, name='Airline Data')\n",
    "display(HTML(airline_profile.to_html()))\n",
    "airline_profile.to_csv(PATH+'profiling/airline_profile.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aircraft Data Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aircraft_profile = profile_dataframe(df_aircraft, name='Aircraft Data')\n",
    "display(HTML(aircraft_profile.to_html()))\n",
    "aircraft_profile.to_csv(PATH+'profiling/aircraft_profile.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights from the data profile results:\n",
    "\n",
    "- there are some `startDate` and `endDate` equal to 1 --> it is supposed to be a year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_aircraft[(df_aircraft['startDate'] < 1000) | (df_aircraft['endDate'] < 1000)]\n",
    "df_filtered.style.map(\n",
    "    lambda val: 'background-color: red' if val < 1000 else '',\n",
    "    subset=['startDate', 'endDate']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airline = pd.read_pickle(PATH + FILTERED_PATH + 'airline.pkl')\n",
    "\n",
    "# Group by 'Month' and sum 'Flt'\n",
    "monthly_flt_sum = df_airline.groupby('Month')['Flt'].sum().reset_index()\n",
    "\n",
    "# Sort by month to be sure\n",
    "monthly_flt_sum = monthly_flt_sum.sort_values('Month')\n",
    "\n",
    "# Map month numbers to names (Jan, Feb, ...)\n",
    "month_names = [calendar.month_abbr[m] for m in monthly_flt_sum['Month']]\n",
    "monthly_flt_sum['Month_Name'] = month_names\n",
    "\n",
    "# Display result\n",
    "print(monthly_flt_sum)\n",
    "\n",
    "# Histogram\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(monthly_flt_sum['Month_Name'], monthly_flt_sum['Flt'], color='skyblue', edgecolor='black')\n",
    "\n",
    "# Labels and title\n",
    "plt.title('Total Flights per Month (All Years)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Flights')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Box Plot\n",
    "# Map numeric month to abbreviation\n",
    "df_airline['Month_Name'] = df_airline['Month'].apply(lambda x: calendar.month_abbr[x])\n",
    "\n",
    "# Optional: Order months correctly\n",
    "month_order = list(calendar.month_abbr)[1:]  # ['Jan', 'Feb', ..., 'Dec']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df_airline, x='Month_Name', y='Flt', order=month_order, palette='pastel')\n",
    "\n",
    "# Labels and title\n",
    "plt.title('Distribution of Flights per Month (All Years)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Flights')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A blocking strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 1. Data Loading ===\n",
    "# Caricamento dei dataset\n",
    "df_aircraft = pd.read_csv('data_sources/combined_aircraft_data.csv')\n",
    "df_ntsb = pd.read_pickle(PATH + FILTERED_PATH + 'ntsb.pkl')\n",
    "\n",
    "# Selezione delle colonne necessarie\n",
    "df_ntsb_model = df_ntsb[['NtsbNumber', 'EventDate', 'Vehicles.SerialNumber',\n",
    "                         'Vehicles.RegistrationNumber', 'Vehicles.Make', 'Vehicles.Model']].copy()\n",
    "\n",
    "# === 2. Data Cleaning and Normalization ===\n",
    "def clean_text(s):\n",
    "    \"\"\" Normalizzazione del testo: rimozione di caratteri speciali, lowercase e spazi extra. \"\"\"\n",
    "    return re.sub(r'\\W+', ' ', str(s)).lower().strip()\n",
    "\n",
    "# Pulizia dei dati\n",
    "df_ntsb_model['Vehicles.Model'] = df_ntsb_model['Vehicles.Model'].apply(clean_text)\n",
    "df_ntsb_model['Vehicles.Make'] = df_ntsb_model['Vehicles.Make'].apply(clean_text)\n",
    "\n",
    "df_aircraft['model_no'] = df_aircraft['model_no'].apply(clean_text)\n",
    "df_aircraft['manufacturer_code'] = df_aircraft['manufacturer_code'].apply(clean_text)\n",
    "\n",
    "df_ntsb_model.dropna(subset=['Vehicles.Model'], inplace=True)\n",
    "df_aircraft.dropna(subset=['model_no'], inplace=True)\n",
    "\n",
    "# === 3. Similarity Setup ===\n",
    "jw = sm.JaroWinkler()\n",
    "lev = sm.Levenshtein()\n",
    "jac = sm.Jaccard()  \n",
    "\n",
    "# === 4. Precomputation degli n-gram ===\n",
    "def generate_qgrams(model):\n",
    "    \"\"\" Genera un insieme di trigrammi (q-grams di lunghezza 3) per una stringa data. \"\"\"\n",
    "    qgrams = [model[i:i+3] for i in range(len(model) - 2)]\n",
    "    return set(qgrams)\n",
    "\n",
    "# === 5. Matching with Optimized Loop ===\n",
    "matches = []\n",
    "matched_set = set()  # Set per controllare i duplicati di NtsbNumber + SerialNumber\n",
    "serial_set = set()   # Set per controllare i duplicati di SerialNumber\n",
    "\n",
    "for i, ntsb_row in df_ntsb_model.iterrows():\n",
    "    model_ntsb = ntsb_row['Vehicles.Model']\n",
    "    make_ntsb = ntsb_row['Vehicles.Make']\n",
    "    grams_ntsb = generate_qgrams(model_ntsb)\n",
    "\n",
    "    # ðŸ”Ž **Filtro preliminare basato sul Make (flessibile)**\n",
    "    filtered_aircraft = df_aircraft[\n",
    "        df_aircraft['manufacturer_code'].apply(lambda x: make_ntsb in x or x in make_ntsb or jw.get_sim_score(x, make_ntsb) > 0.85)\n",
    "    ]\n",
    "    \n",
    "    # Se non ci sono candidati, passa al prossimo ciclo\n",
    "    if filtered_aircraft.empty:\n",
    "        continue\n",
    "\n",
    "    # Precomputa gli n-gram per i candidati\n",
    "    aircraft_grams = {index: generate_qgrams(model) for index, model in enumerate(filtered_aircraft['model_no'])}\n",
    "\n",
    "    # ðŸ”Ž **Filtro preliminare basato sugli n-grammi**\n",
    "    candidate_matches = []\n",
    "    for idx, grams_aircraft in aircraft_grams.items():\n",
    "        if len(grams_ntsb & grams_aircraft) >= 2:\n",
    "            candidate_matches.append(filtered_aircraft.index[idx])\n",
    "\n",
    "    if not candidate_matches:\n",
    "        continue  # Nessun match possibile, passo al successivo\n",
    "\n",
    "    # ðŸ”Ž **Controllo diretto:** se esiste un match esatto tra i candidati\n",
    "    direct_match = df_aircraft.loc[candidate_matches]\n",
    "    direct_match = direct_match[direct_match['model_no'] == model_ntsb]\n",
    "\n",
    "    if not direct_match.empty:\n",
    "        for _, row in direct_match.iterrows():\n",
    "            match_id = f\"{ntsb_row['NtsbNumber']}_{ntsb_row['Vehicles.SerialNumber']}_{row['model_no']}\"\n",
    "            if match_id not in matched_set and ntsb_row['Vehicles.SerialNumber'] not in serial_set:\n",
    "                matches.append({\n",
    "                    'NtsbNumber': ntsb_row['NtsbNumber'],\n",
    "                    'EventDate': ntsb_row['EventDate'],\n",
    "                    'Vehicles.SerialNumber': ntsb_row['Vehicles.SerialNumber'],\n",
    "                    'Vehicles.RegistrationNumber': ntsb_row['Vehicles.RegistrationNumber'],\n",
    "                    'Vehicles.Make': ntsb_row['Vehicles.Make'],\n",
    "                    'Vehicles.Model': model_ntsb,\n",
    "                    'Matched_Aircraft_Model': row['model_no'],\n",
    "                    'engine_count': row['engine_count'],\n",
    "                    'engine_type': row['engine_type'],\n",
    "                    'JW_Score': 1.0,\n",
    "                    'LEV_Score': 1.0,\n",
    "                    'Jac_Score': 1.0,\n",
    "                    'SimilarityScore': 1.0\n",
    "                })\n",
    "                matched_set.add(match_id)\n",
    "                serial_set.add(ntsb_row['Vehicles.SerialNumber'])\n",
    "\n",
    "        continue  # Salta il loop di matching\n",
    "\n",
    "    # ðŸ”Ž **Controllo di Variante Generico**\n",
    "    for idx in candidate_matches:\n",
    "        model_aircraft = df_aircraft.loc[idx, 'model_no']\n",
    "\n",
    "        # Numeric Filter: numbers must match if present\n",
    "        nums_ntsb = re.findall(r'\\d+', model_ntsb)\n",
    "        nums_aircraft = re.findall(r'\\d+', model_aircraft)\n",
    "\n",
    "        if nums_ntsb and nums_aircraft and nums_ntsb != nums_aircraft:\n",
    "            continue\n",
    "\n",
    "        # Computing the Three Similarity Scores\n",
    "        jw_score = jw.get_sim_score(model_ntsb, model_aircraft)\n",
    "        lev_score = lev.get_sim_score(model_ntsb, model_aircraft)\n",
    "        jac_score = jac.get_sim_score(list(grams_ntsb), list(generate_qgrams(model_aircraft)))\n",
    "\n",
    "        # Linear Rule\n",
    "        final_score = 0.4 * jw_score + 0.3 * lev_score + 0.3 * jac_score\n",
    "\n",
    "        # Controllo duplicati\n",
    "        match_id = f\"{ntsb_row['NtsbNumber']}_{ntsb_row['Vehicles.SerialNumber']}_{model_aircraft}\"\n",
    "        if final_score > 0.75 and match_id not in matched_set and ntsb_row['Vehicles.SerialNumber'] not in serial_set:\n",
    "            matches.append({\n",
    "                'NtsbNumber': ntsb_row['NtsbNumber'],\n",
    "                'EventDate': ntsb_row['EventDate'],\n",
    "                'Vehicles.SerialNumber': ntsb_row['Vehicles.SerialNumber'],\n",
    "                'Vehicles.RegistrationNumber': ntsb_row['Vehicles.RegistrationNumber'],\n",
    "                'Vehicles.Make': ntsb_row['Vehicles.Make'],\n",
    "                'Vehicles.Model': model_ntsb,\n",
    "                'Matched_Aircraft_Model': model_aircraft,\n",
    "                'engine_count': df_aircraft.loc[idx, 'engine_count'],\n",
    "                'engine_type': df_aircraft.loc[idx, 'engine_type'],\n",
    "                'JW_Score': round(jw_score, 3),\n",
    "                'LEV_Score': round(lev_score, 3),\n",
    "                'Jac_Score': round(jac_score, 3),\n",
    "                'SimilarityScore': round(final_score, 4)\n",
    "            })\n",
    "            matched_set.add(match_id)\n",
    "            serial_set.add(ntsb_row['Vehicles.SerialNumber'])\n",
    "\n",
    "# === 6. Final Output ===\n",
    "if not matches:\n",
    "    print(\"No matches found with the current rules.\")\n",
    "else:\n",
    "    df_matches = pd.DataFrame(matches)\n",
    "    print(f\"Matches Found: {len(df_matches)}\")\n",
    "    print(\"Columns:\", df_matches.columns.tolist())\n",
    "    df_matches = df_matches.sort_values(by='SimilarityScore', ascending=False)\n",
    "    display(df_matches.head(25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.to_csv(PATH+\"binding/matched_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Individual Challenge: Data Cleaning Expert***\n",
    "\n",
    "*Manuel Cardoso 56274*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicated dataset: NTSB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_sources/ntsb-us-2003-2023.json\", 'r', encoding='utf-8') as f:\n",
    "    ic_ntsb = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### THE CODE ON THE START OF THE CELL BELOW IS COPIED FROM THE START OF THE PROJECT JUST TO HAVE AN EQUAL DATASET, THE INDIVIDUAL CHALLENGE WILL BE DONE WITH THE DATA THAT REMAINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of starting entries: 23403\n",
      "1\n",
      "Number of entries after introducing missing values: 23403\n",
      "Number of entries after introducing duplicate records: 24573\n",
      "Number of entries after introducing invalid values: 24573\n",
      "Number of entries after introducing outliers in 'Longitude' and 'Latitude': 24573\n"
     ]
    }
   ],
   "source": [
    "# Each record is one accident/incident entry in a list\n",
    "\n",
    "# Convert to a DataFrame (this will flatten top-level fields)\n",
    "# For nested fields like 'Vehicles', we might do a separate flatten later\n",
    "messy_ntsb = pd.json_normalize(ic_ntsb, \n",
    "                            meta=[\n",
    "                                'Oid','MKey','Closed','CompletionStatus','HasSafetyRec',\n",
    "                                'HighestInjury','IsStudy','Mode','NtsbNumber',\n",
    "                                'OriginalPublishedDate','MostRecentReportType','ProbableCause',\n",
    "                                'City','Country','EventDate','State','Agency','BoardLaunch',\n",
    "                                'BoardMeetingDate','DocketDate','EventType','Launch','ReportDate',\n",
    "                                'ReportNum','ReportType','AirportId','AirportName','AnalysisNarrative',\n",
    "                                'FactualNarrative','PrelimNarrative','FatalInjuryCount','MinorInjuryCount',\n",
    "                                'SeriousInjuryCount','InvestigationClass','AccidentSiteCondition',\n",
    "                                'Latitude','Longitude','DocketOriginalPublishDate'\n",
    "                            ],\n",
    "                            record_path=['Vehicles'],  # This flattens out the 'Vehicles' array\n",
    "                            record_prefix='Vehicles.'\n",
    "                           )\n",
    "\n",
    "# combines all injury counts to 1 column\n",
    "messy_ntsb['TotalInjuryCount'] = messy_ntsb[['FatalInjuryCount', 'MinorInjuryCount', 'SeriousInjuryCount']].sum(axis=1)\n",
    "\n",
    "# dropping unnecessary columns\n",
    "messy_ntsb.drop(columns=['AnalysisNarrative','FactualNarrative','PrelimNarrative','InvestigationClass','BoardLaunch'\n",
    "                      ,'BoardMeetingDate','Launch','IsStudy','OriginalPublishedDate','DocketOriginalPublishDate'\n",
    "                      ,'ReportType','ReportNum','ReportDate','MostRecentReportType','FatalInjuryCount','MinorInjuryCount'\n",
    "                      ,'SeriousInjuryCount','DocketDate','Mode','HasSafetyRec','CompletionStatus','Closed'\n",
    "                      ,'Vehicles.AircraftCategory','Vehicles.AmateurBuilt','Vehicles.EventID','Vehicles.AirMedical'\n",
    "                      ,'Vehicles.AirMedicalType','Vehicles.flightScheduledType','Vehicles.flightServiceType'\n",
    "                      ,'Vehicles.flightTerminalType','Vehicles.RegisteredOwner','Vehicles.RegulationFlightConductedUnder'\n",
    "                      ,'Vehicles.RepGenFlag','Vehicles.RevenueSightseeing','Vehicles.SecondPilotPresent','Vehicles.Damage'\n",
    "                      ,'AccidentSiteCondition'], inplace=True) \n",
    "\n",
    "# dropping NaT entries from EventDate\n",
    "messy_ntsb = messy_ntsb.dropna(subset=['EventDate'])\n",
    "\n",
    "# Type Conversion\n",
    "messy_ntsb['EventDate'] = pd.to_datetime(messy_ntsb['EventDate']).dt.tz_localize(None)\n",
    "messy_ntsb['Vehicles.VehicleNumber'] = pd.to_numeric(messy_ntsb['Vehicles.VehicleNumber'], errors='coerce').astype(int)\n",
    "messy_ntsb['MKey'] = pd.to_numeric(messy_ntsb['MKey'], errors='coerce').astype(int)\n",
    "messy_ntsb['Vehicles.NumberOfEngines'] = pd.to_numeric(messy_ntsb['Vehicles.NumberOfEngines'], errors='coerce').fillna(0).astype(float) # only change from the original Data Cleaning so it was easier to manipulate\n",
    "messy_ntsb['Latitude'] = pd.to_numeric(messy_ntsb['Latitude'], errors='coerce').astype(float)\n",
    "messy_ntsb['Longitude'] = pd.to_numeric(messy_ntsb['Longitude'], errors='coerce').astype(float)\n",
    "messy_ntsb['TotalInjuryCount'] = pd.to_numeric(messy_ntsb['TotalInjuryCount'], errors='coerce').astype(int)\n",
    "\n",
    "categorical_cols = [\n",
    "    'Vehicles.DamageLevel',\n",
    "    'Vehicles.ExplosionType',\n",
    "    'Vehicles.FireType',\n",
    "    'HighestInjury',\n",
    "    'EventType',\n",
    "    'AccidentSiteCondition'\n",
    "]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in messy_ntsb.columns:\n",
    "        messy_ntsb[col] = messy_ntsb[col].astype('category')\n",
    "\n",
    "messy_ntsb = messy_ntsb.map(lambda x: x.lower() if isinstance(x, str) else x) # make all appropriate values lowercase\n",
    "\n",
    "#################################################################################### CODE ABOVE IS COPIED ####################################################################################\n",
    "\n",
    "starting_entries = len(messy_ntsb)\n",
    "print(f\"Number of starting entries: {starting_entries}\")\n",
    "\n",
    "# Manipulating records\n",
    "seed = 5 # for reproducibility\n",
    "\n",
    "# Introducing missing values on Vehicles.Make column\n",
    "missing_count = messy_ntsb['Vehicles.Make'].isna().sum()\n",
    "print(missing_count)\n",
    "np.random.seed(seed) \n",
    "n = starting_entries\n",
    "n_missing = int(np.floor(0.1 * n)) # 0.1 = 10% missing values\n",
    "missing_indices = np.random.choice(messy_ntsb.index, n_missing, replace=False)\n",
    "messy_ntsb.loc[missing_indices, \"Vehicles.Make\"] = np.nan\n",
    "print(f\"Number of entries after introducing missing values: {len(messy_ntsb)}\")\n",
    "\n",
    "# Introducing duplicate records\n",
    "n_dup = int(np.floor(0.05 * n)) # 0.05 = 5% duplicated records\n",
    "# Randomly choose rows to duplicate\n",
    "dup_indices = np.random.choice(messy_ntsb.index, n_dup, replace=False)\n",
    "duplicates = messy_ntsb.loc[dup_indices].copy()\n",
    "# Append duplicates to original DataFrame\n",
    "messy_ntsb = pd.concat([messy_ntsb, duplicates], ignore_index=True)\n",
    "print(f\"Number of entries after introducing duplicate records: {len(messy_ntsb)}\")\n",
    "n = len(messy_ntsb) # reset\n",
    "\n",
    "# Introducing negative and incorrect values for Vehicles.NumberOfEngines\n",
    "n_invalid = int(np.floor(0.05 * n)) # 0.05 = 5% induced negatives and incorrect \n",
    "# Randomly choose rows\n",
    "invalid_indices = np.random.choice(messy_ntsb.index, n_invalid, replace=False)\n",
    "# Flip values to negative (ensure they're numeric first)\n",
    "messy_ntsb.loc[invalid_indices, \"Vehicles.NumberOfEngines\"] = -messy_ntsb.loc[invalid_indices, \"Vehicles.NumberOfEngines\"].abs()\n",
    "# Randomly choose rows\n",
    "invalid_indices = np.random.choice(messy_ntsb.index, n_invalid, replace=False)\n",
    "# Flip values to negative (ensure they're numeric first)\n",
    "messy_ntsb.loc[invalid_indices, \"Vehicles.NumberOfEngines\"] = (messy_ntsb.loc[invalid_indices, \"Vehicles.NumberOfEngines\"] + (0.01*np.random.rand())) # add decimals to the Number of Engines\n",
    "print(f\"Number of entries after introducing invalid values: {len(messy_ntsb)}\")\n",
    "\n",
    "# Introducing Outliers in 'Longitude' and 'Latitude' columns\n",
    "n_outliers = int(np.floor(0.025 * n)) # 0.025 = 2.5% induced outliers\n",
    "outlier_indices = np.random.choice(messy_ntsb.index, n_outliers, replace=False)\n",
    "# Longitude: valid range ~ -180 to 180\n",
    "messy_ntsb.loc[outlier_indices, 'Longitude'] = (messy_ntsb.loc[outlier_indices, 'Longitude'] + np.random.uniform(400, 500, size=n_outliers)) # clearly invalid, just to induce\n",
    "# Latitude: valid range ~ -90 to 90\n",
    "messy_ntsb.loc[outlier_indices, 'Latitude'] = (messy_ntsb.loc[outlier_indices, 'Latitude'] + np.random.uniform(200, 300, size=n_outliers)) # clearly invalid, just to induce\n",
    "print(f\"Number of entries after introducing outliers in 'Longitude' and 'Latitude': {len(messy_ntsb)}\")\n",
    "\n",
    "#messy_ntsb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "#### For this challenge, I'm going to assume that the user noticed the errors on the specific columns and dealt with them (Qualitative Cleaning - \"Manual crafting of rules and transform function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries after dropping duplicates: 23657\n",
      "----------\n",
      "Number of NaN: 2359\n",
      "Number of entries after fixing Vehicles.Make: 23657\n",
      "Number of NaN after trying to fix Vehicles.Make: 154\n",
      "----------\n",
      "Number of negatives: 1198\n",
      "Number of numbers with decimal parts: 1226\n",
      "Number of negatives after trying to fix Vehicles.NumberOfEngines: 0\n",
      "Number of numbers with decimal parts after trying to fix Vehicles.NumberOfEngines: 0\n",
      "----------\n",
      "Number of outliers in Longitude: 613\n",
      "Number of outliers in Latitude: 612\n",
      "Number of outliers in Longitude after fix: 0\n",
      "Number of outliers in Latitude after fix: 0\n",
      "----------\n",
      "Number of entries after dropping duplicates at the end: 23466\n",
      "Number of starting entries for comparison: 23403\n"
     ]
    }
   ],
   "source": [
    "# This block of code needs the previous block of code to be ran first\n",
    "\n",
    "# First, start by dropping duplicate rows (this will have to be done again at the end, we do it at the start anyway to minimize computing needs)\n",
    "messy_ntsb = messy_ntsb.drop_duplicates()\n",
    "print(f\"Number of entries after dropping duplicates: {len(messy_ntsb)}\")\n",
    "\n",
    "print(\"----------\")\n",
    "\n",
    "# Fixing missing values in column Vehicles.Make\n",
    "# Filter rows where 'Vehicles.Make' is not missing\n",
    "model_dict = {}\n",
    "missing_count = messy_ntsb['Vehicles.Make'].isna().sum()\n",
    "print(f\"Number of NaN: {missing_count}\")\n",
    "for index, row in messy_ntsb.iterrows():\n",
    "    make = row['Vehicles.Make']\n",
    "    model = row['Vehicles.Model']\n",
    "    \n",
    "    if pd.notna(make):  # Only build dict from known makes\n",
    "        model_dict[model] = make\n",
    "\n",
    "# Iterate again to replace NaNs\n",
    "for index, row in messy_ntsb.iterrows():\n",
    "    make = row['Vehicles.Make']\n",
    "    model = row['Vehicles.Model']\n",
    "    \n",
    "    if pd.isna(make):  # Only build dict from known makes\n",
    "        messy_ntsb.loc[index, 'Vehicles.Make'] = model_dict.get(model, None)\n",
    "\n",
    "\n",
    "print(f\"Number of entries after fixing Vehicles.Make: {len(messy_ntsb)}\")\n",
    "\n",
    "missing_count = messy_ntsb['Vehicles.Make'].isna().sum()\n",
    "print(f\"Number of NaN after trying to fix Vehicles.Make: {missing_count}\") \n",
    "# We can check that not all was fixed, I tried to fix by checking with other entries that had the same Model\n",
    "# but if a Model never has a Make to begin with, this can't be done\n",
    "\n",
    "print(\"----------\")\n",
    "\n",
    "# Fixing invalid values in Vehicles.NumberOfEngines\n",
    "engines_dict = {}\n",
    "negative_count = (messy_ntsb['Vehicles.NumberOfEngines'] < 0).sum()\n",
    "decimal_count = (messy_ntsb['Vehicles.NumberOfEngines'] % 1 != 0).sum() - messy_ntsb['Vehicles.NumberOfEngines'].isna().sum() # NaNs count here if not for the subtraction\n",
    "print(f\"Number of negatives: {negative_count}\")\n",
    "print(f\"Number of numbers with decimal parts: {decimal_count}\")\n",
    "for index, row in messy_ntsb.iterrows():\n",
    "    engines = row['Vehicles.NumberOfEngines']\n",
    "    model = row['Vehicles.Model']\n",
    "\n",
    "    if engines.is_integer() and engines >= 1:\n",
    "        engines_dict[model] = engines\n",
    "\n",
    "    elif engines < 0:\n",
    "        messy_ntsb.loc[index, 'Vehicles.NumberOfEngines'] = abs(engines)\n",
    "\n",
    "# Iterate again to check for the ones with decimal parts and previously negative\n",
    "for index, row in messy_ntsb.iterrows():\n",
    "    model = row['Vehicles.Model']\n",
    "\n",
    "    if model in engines_dict:\n",
    "        messy_ntsb.loc[index, 'Vehicles.NumberOfEngines'] = engines_dict[model]\n",
    "    else:\n",
    "        messy_ntsb.loc[index, 'Vehicles.NumberOfEngines'] = None # I could round the number to the closet whole number but that wouldn't be trustworthy \n",
    "                                                                 # so I think it's better to replace it with None\n",
    "\n",
    "\n",
    "negative_count = (messy_ntsb['Vehicles.NumberOfEngines'] < 0).sum()\n",
    "decimal_count = (messy_ntsb['Vehicles.NumberOfEngines'] % 1 != 0).sum() - messy_ntsb['Vehicles.NumberOfEngines'].isna().sum()\n",
    "print(f\"Number of negatives after trying to fix Vehicles.NumberOfEngines: {negative_count}\")\n",
    "print(f\"Number of numbers with decimal parts after trying to fix Vehicles.NumberOfEngines: {decimal_count}\")\n",
    "\n",
    "messy_ntsb['Vehicles.NumberOfEngines'] = pd.to_numeric(messy_ntsb['Vehicles.NumberOfEngines'], errors='coerce').fillna(0).astype(int) # fix Type Conversion\n",
    "\n",
    "print(\"----------\")\n",
    "\n",
    "# Removing outliers from Longitude and Latitude ~ -180 to 180\n",
    "longitude_count = (messy_ntsb['Longitude'] < -180).sum() + (messy_ntsb['Longitude'] > 180).sum()\n",
    "latitude_count = (messy_ntsb['Latitude'] < -90).sum() + (messy_ntsb['Latitude'] > 90).sum()\n",
    "print(f\"Number of outliers in Longitude: {longitude_count}\")\n",
    "print(f\"Number of outliers in Latitude: {latitude_count}\")\n",
    "for index, row in messy_ntsb.iterrows():\n",
    "    long = row['Longitude']\n",
    "    lat = row['Latitude']\n",
    "\n",
    "    if long and (long < -180 or long > 190):\n",
    "        messy_ntsb.loc[index, 'Longitude'] = None\n",
    "\n",
    "    if lat and (lat < -90 or lat > 90):\n",
    "        messy_ntsb.loc[index, 'Latitude'] = None\n",
    "\n",
    "longitude_count = (messy_ntsb['Longitude'] < -180).sum() + (messy_ntsb['Longitude'] > 180).sum()\n",
    "latitude_count = (messy_ntsb['Latitude'] < -90).sum() + (messy_ntsb['Latitude'] > 90).sum()\n",
    "print(f\"Number of outliers in Longitude after fix: {longitude_count}\")\n",
    "print(f\"Number of outliers in Latitude after fix: {latitude_count}\")\n",
    "\n",
    "print(\"----------\")\n",
    "\n",
    "messy_ntsb = messy_ntsb.drop_duplicates()\n",
    "print(f\"Number of entries after dropping duplicates at the end: {len(messy_ntsb)}\") # we do this again because there may exist rows that had records manipulated and weren't \n",
    "                                                                                    # duplicated because of that, but at the end of the Data Cleaning could be duplicated again\n",
    "print(f\"Number of starting entries for comparison: {starting_entries}\") # this value being different is natural, as the order of the manipulation has the duplication happening before other\n",
    "                                                                        # data issue insertions,, I tested with the duplication as the last manipulation and the number of entries\n",
    "                                                                        # at the end of the Data Cleaning coincides with the starting entries (23403) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
