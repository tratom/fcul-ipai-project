{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "#import pymongo as pm\n",
    "#import mysql.connector\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import py_stringmatching as sm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "1. Load the `.csv` and `.json` dataset;\n",
    "2. Drop the rows that do not contains required data\n",
    "3. Fill the `na` cells with a predefined value\n",
    "4. Drop eventualy doplicates\n",
    "5. Convert the string data into the proper data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check NA values presence before data validation\n",
      "Airline traffic data frame: False\n",
      "NTSB data frame: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Dom_Pax</th>\n",
       "      <th>Int_Pax</th>\n",
       "      <th>Pax</th>\n",
       "      <th>Dom_Flt</th>\n",
       "      <th>Int_Flt</th>\n",
       "      <th>Flt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "      <td>43,032,450</td>\n",
       "      <td>4,905,830</td>\n",
       "      <td>47,938,280</td>\n",
       "      <td>785,160</td>\n",
       "      <td>57,667</td>\n",
       "      <td>842,827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>41,166,780</td>\n",
       "      <td>4,245,366</td>\n",
       "      <td>45,412,146</td>\n",
       "      <td>690,351</td>\n",
       "      <td>51,259</td>\n",
       "      <td>741,610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003</td>\n",
       "      <td>3</td>\n",
       "      <td>49,992,700</td>\n",
       "      <td>5,008,613</td>\n",
       "      <td>55,001,313</td>\n",
       "      <td>797,194</td>\n",
       "      <td>58,926</td>\n",
       "      <td>856,120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>4</td>\n",
       "      <td>47,033,260</td>\n",
       "      <td>4,345,444</td>\n",
       "      <td>51,378,704</td>\n",
       "      <td>766,260</td>\n",
       "      <td>55,005</td>\n",
       "      <td>821,265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003</td>\n",
       "      <td>5</td>\n",
       "      <td>49,152,352</td>\n",
       "      <td>4,610,834</td>\n",
       "      <td>53,763,186</td>\n",
       "      <td>789,397</td>\n",
       "      <td>55,265</td>\n",
       "      <td>844,662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>2023</td>\n",
       "      <td>5</td>\n",
       "      <td>71,423,653</td>\n",
       "      <td>10,358,666</td>\n",
       "      <td>81,782,319</td>\n",
       "      <td>667,331</td>\n",
       "      <td>71,924</td>\n",
       "      <td>739,255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>2023</td>\n",
       "      <td>6</td>\n",
       "      <td>72,482,621</td>\n",
       "      <td>11,544,505</td>\n",
       "      <td>84,027,126</td>\n",
       "      <td>661,293</td>\n",
       "      <td>75,279</td>\n",
       "      <td>736,572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>2023</td>\n",
       "      <td>7</td>\n",
       "      <td>75,378,157</td>\n",
       "      <td>12,432,615</td>\n",
       "      <td>87,810,772</td>\n",
       "      <td>684,939</td>\n",
       "      <td>79,738</td>\n",
       "      <td>764,677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>71,477,988</td>\n",
       "      <td>11,572,149</td>\n",
       "      <td>83,050,137</td>\n",
       "      <td>691,482</td>\n",
       "      <td>77,137</td>\n",
       "      <td>768,619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "      <td>66,858,490</td>\n",
       "      <td>9,392,985</td>\n",
       "      <td>76,251,475</td>\n",
       "      <td>649,308</td>\n",
       "      <td>64,241</td>\n",
       "      <td>713,549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year  Month     Dom_Pax     Int_Pax         Pax  Dom_Flt Int_Flt      Flt\n",
       "0    2003      1  43,032,450   4,905,830  47,938,280  785,160  57,667  842,827\n",
       "1    2003      2  41,166,780   4,245,366  45,412,146  690,351  51,259  741,610\n",
       "2    2003      3  49,992,700   5,008,613  55,001,313  797,194  58,926  856,120\n",
       "3    2003      4  47,033,260   4,345,444  51,378,704  766,260  55,005  821,265\n",
       "4    2003      5  49,152,352   4,610,834  53,763,186  789,397  55,265  844,662\n",
       "..    ...    ...         ...         ...         ...      ...     ...      ...\n",
       "244  2023      5  71,423,653  10,358,666  81,782,319  667,331  71,924  739,255\n",
       "245  2023      6  72,482,621  11,544,505  84,027,126  661,293  75,279  736,572\n",
       "246  2023      7  75,378,157  12,432,615  87,810,772  684,939  79,738  764,677\n",
       "247  2023      8  71,477,988  11,572,149  83,050,137  691,482  77,137  768,619\n",
       "248  2023      9  66,858,490   9,392,985  76,251,475  649,308  64,241  713,549\n",
       "\n",
       "[249 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = 'data_sources'\n",
    "\n",
    "# Load dataset into pandas dataframe\n",
    "df_airline_traffic = pd.read_csv(f'{PATH}/u-s-airline-traffic-data.csv')\n",
    "df_ntsb = pd.read_json(f'{PATH}/ntsb-us-2003-2023.json')\n",
    "\n",
    "print('Check NA values presence before data validation')\n",
    "print(f'Airline traffic data frame: {df_airline_traffic.isna().any().any()}')\n",
    "print(f'NTSB data frame: {df_ntsb.isna().any().any()}')\n",
    "\n",
    "#Cleaning df_ntsb\n",
    "# Convert EventDate to datetime and remove timezone\n",
    "df_ntsb['EventDate'] = pd.to_datetime(df_ntsb['EventDate']).dt.tz_localize(None)\n",
    "\n",
    "#df.drop_duplicates(subset=[col for col in df.columns if df[col].dtype != 'object'], inplace=True) # no need to drop duplicates because there aren't\n",
    "\n",
    "df_ntsb = df_ntsb.map(lambda x: x.lower() if isinstance(x, str) else x) # make all appropriate values lowercase\n",
    "\n",
    "# combines all injury counts to 1 column\n",
    "df_ntsb['TotalInjuryCount'] = df_ntsb[['FatalInjuryCount', 'MinorInjuryCount', 'SeriousInjuryCount']].sum(axis=1)\n",
    "\n",
    "# dropping unnecessary columns\n",
    "df_ntsb.drop(columns=['AnalysisNarrative','FactualNarrative','PrelimNarrative','InvestigationClass','BoardLaunch','BoardMeetingDate','Launch','IsStudy'\n",
    "                 ,'OriginalPublishedDate','DocketOriginalPublishDate','ReportType','ReportNum','ReportDate','MostRecentReportType'\n",
    "                 ,'FatalInjuryCount','MinorInjuryCount','SeriousInjuryCount','DocketDate','Mode','HasSafetyRec','CompletionStatus','Closed'], inplace=True) \n",
    "\n",
    "# dropping NaT entries from EventDate\n",
    "df_ntsb = df_ntsb.dropna(subset=['EventDate'])\n",
    "\n",
    "#print(df_ntsb.columns.tolist())\n",
    "#print(df.describe())  # Summary statistics\n",
    "#print(df.info())  # Data types and missing values\n",
    "#print(df.isnull().sum())  # Check missing values\n",
    "\n",
    "#Cleaning df_airline_traffic\n",
    "\n",
    "# dropping unnecessary columns\n",
    "df_airline_traffic.drop(columns=['Dom_RPM','Int_RPM','RPM','Dom_ASM','Int_ASM','ASM','Dom_LF','Int_LF','LF'], inplace=True) \n",
    "\n",
    "df_airline_traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to filter to the date we want\n",
    "\n",
    "# Debug: Check min and max dates\n",
    "print(\"Earliest Date:\", df_ntsb['EventDate'].min())\n",
    "print(\"Latest Date:\", df_ntsb['EventDate'].max())\n",
    "\n",
    "# Define the date range (without timezone)\n",
    "start_date = pd.to_datetime('2003-01-01')\n",
    "end_date = pd.to_datetime('2023-12-31')\n",
    "\n",
    "# Filter the dataset\n",
    "filtered_df = df_ntsb[(df_ntsb['EventDate'] >= start_date) & (df_ntsb['EventDate'] <= end_date) & (df_ntsb['Country'] == 'usa')]\n",
    "print(filtered_df['State'].tolist())\n",
    "filtered_df\n",
    "# Display results\n",
    "#print(f\"Total Records Found: {len(filtered_df)}\")\n",
    "#print(filtered_df[['EventDate', 'HighestInjury', 'Country']].sample(10))  # Show 50 random dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### open-meteo API call test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'latitude': 41.581722, 'longitude': -90.64935, 'generationtime_ms': 0.3064870834350586, 'utc_offset_seconds': 0, 'timezone': 'GMT', 'timezone_abbreviation': 'GMT', 'elevation': 228.0, 'hourly_units': {'time': 'iso8601', 'temperature_2m': '¬∞C', 'relative_humidity_2m': '%', 'dew_point_2m': '¬∞C', 'pressure_msl': 'hPa', 'surface_pressure': 'hPa', 'precipitation': 'mm', 'rain': 'mm', 'snowfall': 'cm', 'cloud_cover': '%', 'cloud_cover_low': '%', 'cloud_cover_mid': '%', 'cloud_cover_high': '%', 'wind_speed_10m': 'km/h', 'wind_speed_100m': 'km/h', 'wind_direction_10m': '¬∞', 'wind_direction_100m': '¬∞', 'wind_gusts_10m': 'km/h', 'weather_code': 'wmo code', 'snow_depth': 'm'}, 'hourly': {'time': ['2023-12-31T00:00', '2023-12-31T01:00', '2023-12-31T02:00', '2023-12-31T03:00', '2023-12-31T04:00', '2023-12-31T05:00', '2023-12-31T06:00', '2023-12-31T07:00', '2023-12-31T08:00', '2023-12-31T09:00', '2023-12-31T10:00', '2023-12-31T11:00', '2023-12-31T12:00', '2023-12-31T13:00', '2023-12-31T14:00', '2023-12-31T15:00', '2023-12-31T16:00', '2023-12-31T17:00', '2023-12-31T18:00', '2023-12-31T19:00', '2023-12-31T20:00', '2023-12-31T21:00', '2023-12-31T22:00', '2023-12-31T23:00'], 'temperature_2m': [1.1, -0.6, -1.0, -0.4, -1.3, -1.8, -2.2, -2.7, -2.7, -2.9, -3.2, -3.1, -2.9, -2.6, -2.4, -1.6, -0.7, -0.1, 0.6, 1.2, 1.0, 1.1, 0.8, 0.1], 'relative_humidity_2m': [85, 89, 89, 80, 71, 69, 70, 74, 73, 71, 72, 76, 78, 77, 77, 74, 70, 73, 74, 73, 72, 68, 67, 70], 'dew_point_2m': [-1.1, -2.2, -2.6, -3.5, -5.8, -6.7, -6.8, -6.7, -6.9, -7.4, -7.5, -6.8, -6.2, -6.1, -5.8, -5.7, -5.6, -4.4, -3.4, -3.1, -3.5, -4.2, -4.5, -4.8], 'pressure_msl': [1011.4, 1012.5, 1012.8, 1013.3, 1013.5, 1013.9, 1014.2, 1014.0, 1014.1, 1014.6, 1014.6, 1014.8, 1015.1, 1015.8, 1016.1, 1016.6, 1017.3, 1017.7, 1017.8, 1018.2, 1018.8, 1019.7, 1020.8, 1021.8], 'surface_pressure': [983.2, 984.0, 984.3, 984.8, 984.9, 985.3, 985.5, 985.3, 985.4, 985.9, 985.8, 986.0, 986.3, 987.1, 987.4, 987.9, 988.7, 989.2, 989.3, 989.8, 990.3, 991.2, 992.3, 993.2], 'precipitation': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0], 'rain': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'snowfall': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21, 0.07, 0.07, 0.0, 0.0, 0.0, 0.0], 'cloud_cover': [100, 87, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 99, 100, 100, 100, 100, 100, 100, 99, 100], 'cloud_cover_low': [0, 46, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 98, 96, 100, 100, 97, 100, 100, 92, 98, 100], 'cloud_cover_mid': [88, 0, 0, 0, 0, 0, 0, 25, 12, 37, 45, 98, 100, 100, 96, 85, 100, 100, 100, 100, 100, 100, 42, 0], 'cloud_cover_high': [100, 76, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 99, 18, 0, 0, 0, 0, 0, 0], 'wind_speed_10m': [12.0, 11.2, 13.2, 16.6, 17.4, 16.7, 15.8, 16.7, 17.8, 18.1, 18.5, 17.7, 17.4, 16.9, 15.9, 16.8, 18.4, 20.4, 22.6, 25.1, 21.5, 20.1, 18.1, 18.0], 'wind_speed_100m': [26.0, 25.5, 27.7, 27.6, 27.5, 26.1, 25.6, 27.9, 28.5, 28.7, 29.3, 27.5, 27.4, 26.7, 24.8, 23.8, 25.5, 28.0, 31.5, 35.4, 30.8, 29.6, 28.6, 30.1], 'wind_direction_10m': [261, 272, 292, 319, 310, 303, 297, 284, 288, 293, 294, 294, 292, 294, 295, 305, 310, 315, 326, 331, 329, 327, 325, 323], 'wind_direction_100m': [266, 279, 300, 320, 312, 305, 299, 287, 291, 297, 298, 297, 295, 297, 300, 308, 312, 315, 327, 332, 330, 328, 326, 326], 'wind_gusts_10m': [19.1, 19.1, 21.2, 26.6, 29.2, 28.8, 28.1, 27.4, 29.9, 30.2, 31.0, 31.7, 29.5, 28.8, 27.7, 29.2, 32.8, 33.8, 39.6, 44.3, 43.2, 37.1, 33.5, 29.5], 'weather_code': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 73, 71, 71, 3, 3, 3, 3], 'snow_depth': [0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02]}}\n",
      "Weather data at 2023-12-31T17:00Z:\n",
      "temperature_2m: -0.1\n",
      "relative_humidity_2m: 73\n",
      "dew_point_2m: -4.4\n",
      "pressure_msl: 1017.7\n",
      "surface_pressure: 989.2\n",
      "precipitation: 0.3\n",
      "rain: 0.0\n",
      "snowfall: 0.21\n",
      "cloud_cover: 100\n",
      "cloud_cover_low: 100\n",
      "cloud_cover_mid: 100\n",
      "cloud_cover_high: 18\n",
      "wind_speed_10m: 20.4\n",
      "wind_speed_100m: 28.0\n",
      "wind_direction_10m: 315\n",
      "wind_direction_100m: 315\n",
      "wind_gusts_10m: 33.8\n",
      "weather_code: 73\n",
      "snow_depth: 0.02\n"
     ]
    }
   ],
   "source": [
    "# Define the endpoint\n",
    "endpoint = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "# Define the parameters\n",
    "params = {\n",
    "    \"latitude\": 41.610278,\n",
    "    \"longitude\": -90.588361,\n",
    "    \"start_date\": \"2023-12-31\",\n",
    "    \"end_date\": \"2023-12-31\",\n",
    "    \"hourly\": \",\".join([\n",
    "        \"temperature_2m\",\n",
    "        \"relative_humidity_2m\",\n",
    "        \"dew_point_2m\",\n",
    "        \"pressure_msl\",\n",
    "        \"surface_pressure\",\n",
    "        \"precipitation\",\n",
    "        \"rain\",\n",
    "        \"snowfall\",\n",
    "        \"cloud_cover\",\n",
    "        \"cloud_cover_low\",\n",
    "        \"cloud_cover_mid\",\n",
    "        \"cloud_cover_high\",\n",
    "        \"wind_speed_10m\",\n",
    "        \"wind_speed_100m\",\n",
    "        \"wind_direction_10m\",\n",
    "        \"wind_direction_100m\",\n",
    "        \"wind_gusts_10m\",\n",
    "        \"weather_code\",\n",
    "        \"snow_depth\"\n",
    "    ]),\n",
    "    \"timezone\": \"GMT\"\n",
    "}\n",
    "\n",
    "# Make the request\n",
    "response = requests.get(endpoint, params=params)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Process the data as needed\n",
    "    print(data)\n",
    "    time_series = data[\"hourly\"][\"time\"]\n",
    "    try:\n",
    "        idx = time_series.index(\"2023-12-31T17:00\")\n",
    "        selected_data = {k: v[idx] for k, v in data[\"hourly\"].items() if k != \"time\"}\n",
    "        print(f\"Weather data at 2023-12-31T17:00Z:\")\n",
    "        for key, val in selected_data.items():\n",
    "            print(f\"{key}: {val}\")\n",
    "    except ValueError:\n",
    "        print(\"Selected hour not found in response.\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####Update the file \n",
    "NTSB_DATA = \"ntsb-us-2003-2023.json\"\n",
    "\n",
    "with open(NTSB_DATA, 'r', encoding='utf-8') as f:\n",
    "    ntsb_raw_data = json.load(f)\n",
    "\n",
    "# Each record is one accident/incident entry in a list\n",
    "print(f'\\n--- NTSB JSON loaded: {len(ntsb_raw_data)} total records found ---')\n",
    "\n",
    "# Convert to a DataFrame (this will flatten top-level fields)\n",
    "# For nested fields like 'Vehicles', we might do a separate flatten later\n",
    "df_ntsb = pd.json_normalize(ntsb_raw_data,\n",
    "                            meta=[\n",
    "                                'Oid','MKey','Closed','CompletionStatus','HasSafetyRec',\n",
    "                                'HighestInjury','IsStudy','Mode','NtsbNumber',\n",
    "                                'OriginalPublishedDate','MostRecentReportType','ProbableCause',\n",
    "                                'City','Country','EventDate','State','Agency','BoardLaunch',\n",
    "                                'BoardMeetingDate','DocketDate','EventType','Launch','ReportDate',\n",
    "                                'ReportNum','ReportType','AirportId','AirportName','AnalysisNarrative',\n",
    "                                'FactualNarrative','PrelimNarrative','FatalInjuryCount','MinorInjuryCount',\n",
    "                                'SeriousInjuryCount','InvestigationClass','AccidentSiteCondition',\n",
    "                                'Latitude','Longitude','DocketOriginalPublishDate'\n",
    "                            ],\n",
    "                            record_path=['Vehicles'],  # This flattens out the 'Vehicles' array\n",
    "                            record_prefix='Vehicles.'\n",
    "                           )\n",
    "\n",
    "print('\\n--- Flattened NTSB DataFrame (including Vehicles info): ---')\n",
    "\n",
    "# combines all injury counts to 1 column\n",
    "df_ntsb['TotalInjuryCount'] = df_ntsb[['FatalInjuryCount', 'MinorInjuryCount', 'SeriousInjuryCount']].sum(axis=1)\n",
    "\n",
    "# dropping unnecessary columns\n",
    "df_ntsb.drop(columns=['AnalysisNarrative','FactualNarrative','PrelimNarrative','InvestigationClass','BoardLaunch'\n",
    "                      ,'BoardMeetingDate','Launch','IsStudy','OriginalPublishedDate','DocketOriginalPublishDate'\n",
    "                      ,'ReportType','ReportNum','ReportDate','MostRecentReportType','FatalInjuryCount','MinorInjuryCount'\n",
    "                      ,'SeriousInjuryCount','DocketDate','Mode','HasSafetyRec','CompletionStatus','Closed'\n",
    "                      ,'Vehicles.AircraftCategory','Vehicles.AmateurBuilt','Vehicles.EventID','Vehicles.AirMedical'\n",
    "                      ,'Vehicles.AirMedicalType','Vehicles.flightScheduledType','Vehicles.flightServiceType'\n",
    "                      ,'Vehicles.flightTerminalType','Vehicles.RegisteredOwner','Vehicles.RegulationFlightConductedUnder'\n",
    "                      ,'Vehicles.RepGenFlag','Vehicles.RevenueSightseeing','Vehicles.SecondPilotPresent','Vehicles.Damage'\n",
    "                      ,'AccidentSiteCondition'], inplace=True)\n",
    "\n",
    "# dropping NaT entries from EventDate\n",
    "df_ntsb = df_ntsb.dropna(subset=['EventDate'])\n",
    "\n",
    "# Type Conversion\n",
    "df_ntsb['EventDate'] = pd.to_datetime(df_ntsb['EventDate']).dt.tz_localize(None)\n",
    "df_ntsb['Vehicles.VehicleNumber'] = pd.to_numeric(df_ntsb['Vehicles.VehicleNumber'], errors='coerce').astype(int)\n",
    "df_ntsb['MKey'] = pd.to_numeric(df_ntsb['MKey'], errors='coerce').astype(int)\n",
    "df_ntsb['Vehicles.NumberOfEngines'] = pd.to_numeric(df_ntsb['Vehicles.NumberOfEngines'], errors='coerce').fillna(0).astype(int)\n",
    "df_ntsb['Latitude'] = pd.to_numeric(df_ntsb['Latitude'], errors='coerce').astype(float)\n",
    "df_ntsb['Longitude'] = pd.to_numeric(df_ntsb['Longitude'], errors='coerce').astype(float)\n",
    "df_ntsb['TotalInjuryCount'] = pd.to_numeric(df_ntsb['TotalInjuryCount'], errors='coerce').astype(int)\n",
    "\n",
    "categorical_cols = [\n",
    "    'Vehicles.DamageLevel',\n",
    "    'Vehicles.ExplosionType',\n",
    "    'Vehicles.FireType',\n",
    "    'HighestInjury',\n",
    "    'EventType',\n",
    "    'AccidentSiteCondition'\n",
    "]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in df_ntsb.columns:\n",
    "        df_ntsb[col] = df_ntsb[col].astype('category')\n",
    "\n",
    "df_ntsb = df_ntsb.map(lambda x: x.lower() if isinstance(x, str) else x) # make all appropriate values lowercase\n",
    "\n",
    "\n",
    "print(df_ntsb.head())\n",
    "\n",
    "print('\\n--- DataFrame Info ---')\n",
    "df_ntsb.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    return re.sub(r'\\W+', ' ', str(s)).lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 1. Dataset Loading ===\n",
    "df_aircraft = pd.read_csv('aircraft_data.csv')  # Assicurati del path\n",
    "df_ntsb_model = df_ntsb[['NtsbNumber', 'EventDate', 'Vehicles.SerialNumber',\n",
    "                         'Vehicles.RegistrationNumber', 'Vehicles.Make', 'Vehicles.Model']].copy()\n",
    "\n",
    "# === 2. Data Cleaning and Normalization ===\n",
    "df_ntsb_model['Vehicles.Model'] = df_ntsb_model['Vehicles.Model'].apply(clean_text)\n",
    "df_aircraft['aircraft'] = df_aircraft['aircraft'].apply(clean_text)\n",
    "\n",
    "df_ntsb_model.dropna(subset=['Vehicles.Model'], inplace=True)\n",
    "df_aircraft.dropna(subset=['aircraft'], inplace=True)\n",
    "\n",
    "# === 3. Similarity Setup ===\n",
    "jw = sm.JaroWinkler()\n",
    "lev = sm.Levenshtein()\n",
    "jac = sm.Jaccard()\n",
    "qgram = sm.QgramTokenizer(qval=3)\n",
    "\n",
    "# === 4. Matching with Q-gram and Numeric Filtering ===\n",
    "df_ntsb_model['qgrams'] = df_ntsb_model['Vehicles.Model'].apply(lambda x: set(qgram.tokenize(x)))\n",
    "df_aircraft['qgrams'] = df_aircraft['aircraft'].apply(lambda x: set(qgram.tokenize(x)))\n",
    "matches = []\n",
    "\n",
    "for i, ntsb_row in df_ntsb_model.iterrows():\n",
    "    model_ntsb = ntsb_row['Vehicles.Model']\n",
    "    grams_ntsb = ntsb_row['qgrams']\n",
    "\n",
    "    for j, aircraft_row in df_aircraft.iterrows():\n",
    "        model_aircraft = aircraft_row['aircraft']\n",
    "        grams_aircraft = aircraft_row['qgrams']\n",
    "\n",
    "        # BLOCKING: at least 2 shared q-grams or a substring match\n",
    "        blocking_pass = (\n",
    "            len(grams_ntsb & grams_aircraft) >= 2 or\n",
    "            model_aircraft in model_ntsb or\n",
    "            model_ntsb in model_aircraft\n",
    "        )\n",
    "\n",
    "        if blocking_pass:\n",
    "            # Numeric Filter: numbers must match if present\n",
    "            nums_ntsb = re.findall(r'\\d+', model_ntsb)\n",
    "            nums_aircraft = re.findall(r'\\d+', model_aircraft)\n",
    "\n",
    "            if nums_ntsb and nums_aircraft and nums_ntsb != nums_aircraft:\n",
    "                continue  # i numeri non coincidono ‚Üí scarto\n",
    "\n",
    "            # Computing the Three Similarity Scores\n",
    "            jw_score = jw.get_sim_score(model_ntsb, model_aircraft)\n",
    "            lev_score = lev.get_sim_score(model_ntsb, model_aircraft)\n",
    "            jac_score = jac.get_sim_score(model_ntsb.split(), model_aircraft.split())\n",
    "\n",
    "            # Linear Rule\n",
    "            final_score = 0.4 * jw_score + 0.3 * lev_score + 0.3 * jac_score\n",
    "\n",
    "            if final_score > 0.75:\n",
    "                matches.append({\n",
    "                    'NtsbNumber': ntsb_row['NtsbNumber'],\n",
    "                    'startDate': aircraft_row['startDate'],\n",
    "                    'Vehicles.SerialNumber': ntsb_row['Vehicles.SerialNumber'],\n",
    "                    'Vehicles.RegistrationNumber': ntsb_row['Vehicles.RegistrationNumber'],\n",
    "                    'Vehicles.Make': ntsb_row['Vehicles.Make'],\n",
    "                    'Vehicles.Model': model_ntsb,\n",
    "                    'Matched_Aircraft': model_aircraft,\n",
    "                    'JW_Score': round(jw_score, 3),        \n",
    "                    'LEV_Score': round(lev_score, 3),\n",
    "                    'JAC_Score': round(jac_score, 3),\n",
    "                    'SimilarityScore': round(final_score, 4)\n",
    "                })\n",
    "\n",
    "# === 5. Final Output ===\n",
    "if not matches:\n",
    "    print(\"‚ö†Ô∏è No matches found with the current rules.\")\n",
    "else:\n",
    "    df_matches = pd.DataFrame(matches)\n",
    "    print(f\"‚úÖ Matches Found: {len(df_matches)}\")\n",
    "    print(\"üì¶ Columns:\", df_matches.columns.tolist())\n",
    "    df_matches = df_matches.sort_values(by='SimilarityScore', ascending=False)\n",
    "    display(df_matches.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_matches.head(38))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
